{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1CUZ0dkOo_F"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qmkj-80IHxnd"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xnMOsbqHz61"
   },
   "source": [
    "# Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds4o1h4WHz9U"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/pix2pix\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITZuApL56Mny"
   },
   "source": [
    "This notebook demonstrates image to image translation using conditional GAN's, as described in [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004). Using this technique we can colorize black and white photos, convert google maps to google earth, etc. Here, we convert building facades to real buildings.\n",
    "\n",
    "In example, we will use the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/), helpfully provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep our example short, we will use a preprocessed [copy](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/) of this dataset, created by the authors of the [paper](https://arxiv.org/abs/1611.07004) above.\n",
    "\n",
    "Each epoch takes around 15 seconds on a single V100 GPU.\n",
    "\n",
    "Below is the output generated after training the model for 200 epochs.\n",
    "\n",
    "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
    "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "631R08uAFAv5"
   },
   "source": [
    "## Load utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "jmeW8vM9FEPT",
    "outputId": "ce86632b-62cc-4b26-c008-c4177913151c"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab/48919022\n",
    "import os\n",
    "from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('utils.py','wb').write(src)\n",
    "# print(os.path.abspath('utils.py'))\n",
    "import utils\n",
    "help(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wifwThPoEj7e",
    "outputId": "f65a02d0-99a9-44db-a281-6733efe594c8"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCRPjQZvGy3a"
   },
   "source": [
    "## Mount the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjK6DENhG0p3",
    "outputId": "21024489-7f08-487b-c526-0addfc5e9193"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive   \n",
    "\n",
    "# mount the google drive to my Colab session\n",
    "drive.mount('/content/gdrive')\n",
    "# use the google drive in my Colab session\n",
    "home_path = '/content/gdrive/Shared drives/Embryo_data'\n",
    "print(os.listdir(home_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmfXjOxb2Unm"
   },
   "source": [
    "## Acquire an output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HebxBiMj2UF4",
    "outputId": "9ff7bf1c-ef69-4164-c024-5c2796173a1a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  max_trial = max(int(n) for n in os.listdir(f'{home_path}/pix2pix_output'))\n",
    "  new_trial = max_trial + 1\n",
    "except ValueError:\n",
    "  new_trial = 0\n",
    "print(f'Creating a new folder for current trial #{new_trial}')\n",
    "TRIAL_PATH = f'{home_path}/pix2pix_output/{new_trial}'\n",
    "os.mkdir(TRIAL_PATH)\n",
    "os.mkdir(f'{TRIAL_PATH}/train')\n",
    "os.mkdir(f'{TRIAL_PATH}/test')\n",
    "CLASSIFY_PATH = f'{TRIAL_PATH}/classify'\n",
    "os.mkdir(CLASSIFY_PATH)\n",
    "os.mkdir(f'{CLASSIFY_PATH}/0')\n",
    "os.mkdir(f'{CLASSIFY_PATH}/1')\n",
    "CHECKPOINT_PATH = f'{TRIAL_PATH}/checkpoints'\n",
    "os.mkdir(CHECKPOINT_PATH)\n",
    "\n",
    "import datetime\n",
    "LOG_DIR=f\"{TRIAL_PATH}/logs\"\n",
    "LOG_SUB_DIR=f'{LOG_DIR}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "os.mkdir(LOG_DIR)\n",
    "os.mkdir(LOG_SUB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets). As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n",
    "\n",
    "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n",
    "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kn-k8kTXuAlv"
   },
   "outputs": [],
   "source": [
    "# _URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
    "\n",
    "# path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
    "#                                       origin=_URL,\n",
    "#                                       extract=True)\n",
    "\n",
    "# PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOfEI3UyHDPQ",
    "outputId": "68bdec9d-6e85-4a64-f345-ff82402a91dc"
   },
   "outputs": [],
   "source": [
    "# Fixing the random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Available high quality data\n",
    "embryo_inds = utils.CLEAN_IDX\n",
    "\n",
    "# Directory of the processed *.npy files\n",
    "processed_path = f'{home_path}/processed'\n",
    "polar_processed_path = f'{processed_path}/polarization'\n",
    "\n",
    "video_time_info, train_embryos, val_embryos, test_embryos = utils.split_train_test_val(home_path, embryo_inds)\n",
    "\n",
    "z_agg_mode = \"Max Z\"\n",
    "get_data_path = lambda data_type : f'{processed_path}/{data_type}/max'\n",
    "pol_path = f'{processed_path}/polarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKx7c7JJHU_Z"
   },
   "outputs": [],
   "source": [
    "# Option to skip data preparation (assuming this was run before)\n",
    "SKIP_LOADING = True\n",
    "\n",
    "# Repeat normal data loading for bf and fluo images\n",
    "for data_type in ['fluo_data', 'bf_data']:\n",
    "    if SKIP_LOADING:\n",
    "        continue\n",
    "\n",
    "    data_path = get_data_path(data_type)\n",
    "    print(f'Loading {data_type} data from {data_path}')\n",
    "\n",
    "    # Set the subpaths to save each partition of the dataset \n",
    "    get_save_path = lambda set_type : f'{processed_path}/pix2pix/{set_type}/{data_type}'\n",
    "    train_path = get_save_path('train')\n",
    "    val_path = get_save_path('val')\n",
    "    test_path = get_save_path('test')\n",
    "    print(f'Saving to {[train_path, val_path, test_path]}')\n",
    "\n",
    "    # Actually create the images\n",
    "    # TODO - confirm normalization is ok for translation\n",
    "    specs = (data_path, pol_path, video_time_info)\n",
    "    utils.save_nps_as_png(train_embryos, train_path, specs, window=2, normalize='per_embryo', pol_subdir=False)\n",
    "    utils.save_nps_as_png(test_embryos, test_path, specs, window=2, normalize='per_embryo', pol_subdir=False)\n",
    "    utils.save_nps_as_png(val_embryos, val_path, specs, window=2, normalize='per_embryo', pol_subdir=False)\n",
    "\n",
    "    # utils.save_nps_as_png(train_embryos, train_path, specs, window=None, normalize='per_timestep', pol_subdir=False)\n",
    "    # utils.save_nps_as_png(test_embryos, test_path, specs, window=None, normalize='per_timestep', pol_subdir=False)\n",
    "    # utils.save_nps_as_png(val_embryos, val_path, specs, window=None, normalize='per_timestep', pol_subdir=False)\n",
    "\n",
    "    # max_train_pixel = utils.get_max_pixel(train_embryos, data_path)\n",
    "    # utils.save_nps_as_png(train_embryos, train_path, specs, window=None, normalize=max_train_pixel, pol_subdir=False)\n",
    "    # utils.save_nps_as_png(test_embryos, test_path, specs, window=None, normalize=max_train_pixel, pol_subdir=False)\n",
    "    # utils.save_nps_as_png(val_embryos, val_path, specs, window=None, normalize=max_train_pixel, pol_subdir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CbTEt448b4R"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 512 # 256\n",
    "IMG_HEIGHT = 512 # 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO9ZAGH5K3SY"
   },
   "outputs": [],
   "source": [
    "def load(bf_file):\n",
    "  # Load the bf_file as the input image\n",
    "  input_image = tf.io.read_file(bf_file)\n",
    "  input_image = tf.image.decode_png(input_image)\n",
    "\n",
    "  # Load the corresponding fluo_file as the real image\n",
    "  if type(bf_file) is str:\n",
    "    fluo_file = bf_file.replace(\"bf_data\",\"fluo_data\")\n",
    "  else:\n",
    "    fluo_file = tf.strings.regex_replace(bf_file, \"bf_data\",\"fluo_data\")\n",
    "  real_image = tf.io.read_file(fluo_file)\n",
    "  real_image = tf.image.decode_png(real_image)\n",
    "\n",
    "  # Convert both to a float32\n",
    "  input_image = tf.cast(input_image, tf.float32)\n",
    "  real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "  # # Obtain labels for embryo-time-polarization\n",
    "  # if type(bf_file) is str:\n",
    "  #     file_name = bf_file.split('/')[-1]\n",
    "  #     file_name = file_name.strip(\".png\")\n",
    "  #     labels = file_name.split('_')[1:]\n",
    "  #     labels = [int(l) for l in labels]\n",
    "  # else:\n",
    "  #     file_name = tf.strings.split(bf_file, sep=\"/\")\n",
    "  #     print(file_name.get_shape())\n",
    "  #     file_name = file_name[-1]\n",
    "  #     file_name = tf.strings.strip(\".png\")\n",
    "  #     labels = tf.strings.split(file_name, sep=\"_\")\n",
    "  #     labels = labels[1:]\n",
    "  #     labels = tf.strings.to_number(labels, out_type=tf.dtypes.int32)\n",
    "\n",
    "  return input_image, real_image, bf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "4OLHMpsQ5aOv",
    "outputId": "a01d5216-db1d-4b85-d783-3610bc83f272"
   },
   "outputs": [],
   "source": [
    "BF_TRAIN_PATH = f'{processed_path}/pix2pix/train/bf_data'\n",
    "inp, re, _ = load(BF_TRAIN_PATH+'/embryo_3_0_0.png')\n",
    "# casting to int for matplotlib to show the image\n",
    "plt.figure()\n",
    "plt.imshow(tf.squeeze(inp/255.0))\n",
    "plt.figure()\n",
    "plt.imshow(tf.squeeze(re/255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwwYQpu9FzDu"
   },
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "  input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn3IwqhiIszt"
   },
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "  cropped_image = tf.image.random_crop(\n",
    "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "\n",
    "  return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muhR2cgbLKWW"
   },
   "outputs": [],
   "source": [
    "# normalizing the images to [-1, 1]\n",
    "\n",
    "def normalize(input_image, real_image):\n",
    "  input_image = (input_image / 127.5) - 1\n",
    "  real_image = (real_image / 127.5) - 1\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVQOjcPVLrUc"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "  # resizing to 286 x 286 x 1\n",
    "  new_size = 286 * 2\n",
    "  input_image, real_image = resize(input_image, real_image, new_size, new_size)\n",
    "\n",
    "  # randomly cropping to 512 x 512 x 1\n",
    "  input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    # random mirroring\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "    real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfAQbzy799UV"
   },
   "source": [
    "As you can see in the images below\n",
    "that they are going through random jittering\n",
    "Random jittering as described in the paper is to\n",
    "\n",
    "1. Resize an image to bigger height and width\n",
    "2. Randomly crop to the target size\n",
    "3. Randomly flip the image horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "n0OGdi6D92kM",
    "outputId": "af061e7a-0fd1-4bcd-987f-5b63f7db7c01"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "  rj_inp, rj_re = random_jitter(inp, re)\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.imshow(tf.squeeze(rj_inp/255.0))\n",
    "  plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyaP4hLJ8b4W"
   },
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "  input_image, real_image, labels = load(image_file)\n",
    "  input_image, real_image = random_jitter(input_image, real_image)\n",
    "  input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "  return input_image, real_image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB3Z6D_zKSru"
   },
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "  input_image, real_image, labels = load(image_file)\n",
    "  input_image, real_image = resize(input_image, real_image,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "  input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "  return input_image, real_image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQHmYSmk8b4b"
   },
   "outputs": [],
   "source": [
    "BF_TRAIN_PATH = f'{processed_path}/pix2pix/train/bf_data'\n",
    "train_dataset = tf.data.Dataset.list_files(BF_TRAIN_PATH+'/*.png')\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS9J0yA58b4g"
   },
   "outputs": [],
   "source": [
    "BF_TEST_PATH = f'{processed_path}/pix2pix/test/bf_data'\n",
    "test_dataset = tf.data.Dataset.list_files(BF_TEST_PATH+'/*.png')\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Build the Generator\n",
    "  * The architecture of generator is a modified U-Net.\n",
    "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
    "  * There are skip connections between the encoder and decoder (as in U-Net).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqqvWxlw8b4l"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R09ATE_SH9P"
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6_uCZCppTh7",
    "outputId": "3d3b820b-00c5-4d44-dbeb-a2fa8fca1b65"
   },
   "outputs": [],
   "source": [
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))\n",
    "print (down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhgDsHClSQzP"
   },
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mz-ahSdsq0Oc",
    "outputId": "37be4fb3-3912-45b7-9302-84c312898eb7"
   },
   "outputs": [],
   "source": [
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "  # inputs = tf.keras.layers.Input(shape=[256,256,1])\n",
    "\n",
    "  # down_stack = [\n",
    "  #   downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "  #   downsample(128, 4), # (bs, 64, 64, 128)\n",
    "  #   downsample(256, 4), # (bs, 32, 32, 256)\n",
    "  #   downsample(512, 4), # (bs, 16, 16, 512)\n",
    "  #   downsample(512, 4), # (bs, 8, 8, 512)\n",
    "  #   downsample(512, 4), # (bs, 4, 4, 512)\n",
    "  #   downsample(512, 4), # (bs, 2, 2, 512)\n",
    "  #   downsample(512, 4), # (bs, 1, 1, 512)\n",
    "  # ]\n",
    "\n",
    "  # up_stack = [\n",
    "  #   upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "  #   upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "  #   upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "  #   upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "  #   upsample(256, 4), # (bs, 32, 32, 512)\n",
    "  #   upsample(128, 4), # (bs, 64, 64, 256)\n",
    "  #   upsample(64, 4), # (bs, 128, 128, 128)\n",
    "  # ]\n",
    "\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[512,512,1])\n",
    "\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False), # (bs, 256, 256, 64)\n",
    "    downsample(128, 4), # (bs, 128, 128, 128)\n",
    "    downsample(256, 4), # (bs, 64, 64, 256)\n",
    "    downsample(512, 4), # (bs, 32, 32, 512)\n",
    "    downsample(512, 4), # (bs, 16, 16, 512)\n",
    "    downsample(512, 4), # (bs, 8, 8, 512)\n",
    "    downsample(512, 4), # (bs, 4, 4, 512)\n",
    "    downsample(512, 4), # (bs, 2, 2, 512)\n",
    "    downsample(512, 4), # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "    upsample(512, 4), # (bs, 32, 32, 1024)\n",
    "    upsample(256, 4), # (bs, 64, 64, 512)\n",
    "    upsample(128, 4), # (bs, 128, 128, 256)\n",
    "    upsample(64, 4), # (bs, 256, 256, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 512, 512, 1)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dIbRPFzjmV85",
    "outputId": "3590c8fd-9b47-43ec-c575-3f89f18b7691"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "U1N1_obwtdQH",
    "outputId": "1bd6c9c4-463b-45c3-b1e9-73ef45bf83fc"
   },
   "outputs": [],
   "source": [
    "gen_output = generator(inp[tf.newaxis,...], training=False)\n",
    "plt.imshow(tf.squeeze(gen_output[0,...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpDPEQXIAiQO"
   },
   "source": [
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
    "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSZbDgESHIV6"
   },
   "source": [
    "The training procedure for the generator is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyhxTuvJyIHV"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlB-XMY5Awj9"
   },
   "source": [
    "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTKZfoaoEF22"
   },
   "source": [
    "## Build the Discriminator\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake.\n",
    "    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  # inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n",
    "  # tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image')\n",
    "  inp = tf.keras.layers.Input(shape=[512, 512, 1], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[512, 512, 1], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "\n",
    "  # down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "  # down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "  # down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
    "\n",
    "  # zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "  # conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "  #                               kernel_initializer=initializer,\n",
    "  #                               use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "  # batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  # leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  # zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "  # last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "  #                               kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "  down1 = downsample(32, 4, False)(x) # (bs, 256, 256, 32)\n",
    "  down2 = downsample(64, 4)(down1) # (bs, 128, 128, 64)\n",
    "  down3 = downsample(128, 4)(down2) # (bs, 64, 64, 128)\n",
    "  down4 = downsample(256, 4)(down3) # (bs, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4) # (bs, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "YHoUui4om-Ev",
    "outputId": "b52c0e29-23ce-43bf-eee4-6d6fe53c833d"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "gDkA05NE6QMs",
    "outputId": "26c18110-6f3a-45cb-ce00-2049f54a25b9"
   },
   "outputs": [],
   "source": [
    "disc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOqg1dhUAWoD"
   },
   "source": [
    "**Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1Xbz5OaLj5C"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ede4p2YELFa"
   },
   "source": [
    "The training procedure for the discriminator is shown below.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS9sHa-1BoAF"
   },
   "source": [
    "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the Optimizers and Checkpoint-saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbHFNexF0x6O"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJnftd5sQsv6"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = CHECKPOINT_PATH\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Generate Images\n",
    "\n",
    "Write a function to plot some images during training.\n",
    "\n",
    "* We pass images from the test dataset to the generator.\n",
    "* The generator will then translate the input image into the output.\n",
    "* Last step is to plot the predictions and **voila!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb0QQFHF-JfS"
   },
   "source": [
    "Note: The `training=True` is intentional here since\n",
    "we want the batch statistics while running the model\n",
    "on the test dataset. If we use training=False, we will get\n",
    "the accumulated statistics learned from the training dataset\n",
    "(which we don't want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_labels(file_name):\n",
    "  # ordered output: idx, timestep, pol\n",
    "  file_name = str(file_name.numpy()[0])\n",
    "  file_name = file_name.split('/')[-1]\n",
    "  file_name = file_name.strip(\".png'\")\n",
    "  labels = file_name.split('_')[1:]\n",
    "  return [int(l) for l in labels]\n",
    "\n",
    "def generate_images(model, test_input, tar, file_name, train_test, epoch):\n",
    "  '''\n",
    "  train_test is 'train' or 'test' or 'classify'\n",
    "  '''\n",
    "\n",
    "  idx, t, pol = generate_labels(file_name)\n",
    "\n",
    "  prediction = model(test_input, training=True) \n",
    "\n",
    "  if train_test == 'classify':\n",
    "    prediction = (prediction * 0.5 + 0.5).numpy()\n",
    "    plt.imsave(f'{TRIAL_PATH}/{train_test}/{pol}/embryo{idx}_t{t}_pol{pol}.png', np.squeeze(prediction))\n",
    "    return\n",
    "\n",
    "  # turn off interactive plotting\n",
    "  # https://stackoverflow.com/questions/15713279/calling-pylab-savefig-without-display-in-ipython\n",
    "  # https://chartio.com/resources/tutorials/how-to-save-a-plot-to-a-file-using-matplotlib/\n",
    "  plt.ioff()\n",
    "  fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "  display_list = [test_input[0], tar[0], prediction[0]]\n",
    "  title = [f'Input Image (idx {idx}, t-step {t}, pol {pol})', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "  for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(np.squeeze(display_list[i] * 0.5 + 0.5))\n",
    "    plt.axis('off')\n",
    "  # plt.show()\n",
    "\n",
    "  # put epoch num first so you can order by epoch more easily\n",
    "  plt.savefig(f'{TRIAL_PATH}/{train_test}/epoch{epoch}_embryo{idx}_t{t}_pol{pol}.png', bbox_inches='tight')\n",
    "  plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fc4NzT-DgEx"
   },
   "outputs": [],
   "source": [
    "for example_input, example_target, example_name in test_dataset.take(1):\n",
    "  generate_images(generator, example_input, example_target, example_name, 'train', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLKOG55MErD0"
   },
   "source": [
    "## Training\n",
    "\n",
    "* For each example input generate an output.\n",
    "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
    "* Next, we calculate the generator and the discriminator loss.\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
    "* Then log the losses to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "# EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNNMDBNH12q-",
    "outputId": "cfb21d7f-e540-4285-c027-e38f6b7dfbd8"
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(LOG_SUB_DIR)\n",
    "os.listdir(LOG_SUB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBKUV2sKXDbY"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    gen_output = generator(input_image, training=True)\n",
    "\n",
    "    disc_real_output = discriminator([input_image, target], training=True)\n",
    "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
    "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
    "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
    "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx7s-vBHFKdh"
   },
   "source": [
    "The actual training loop:\n",
    "\n",
    "* Iterates over the number of epochs.\n",
    "* On each epoch it clears the display, and runs `generate_images` to show it's progress.\n",
    "* On each epoch it iterates over the training dataset, printing a '.' for each example.\n",
    "* It saves a checkpoint every 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 0\n",
    "\n",
    "def fit(train_ds, epochs, test_ds):\n",
    "  global MAX_EPOCH\n",
    "  for epoch in range(epochs):\n",
    "    MAX_EPOCH = epoch\n",
    "    start = time.time()\n",
    "\n",
    "    for example_input, example_target, example_name in test_ds.take(5):\n",
    "      generate_images(generator, example_input, example_target, example_name, 'train', epoch)\n",
    "    print(\"Epoch: \", epoch)\n",
    "\n",
    "    # Train\n",
    "    for n, (input_image, target, name) in train_ds.enumerate():\n",
    "      train_step(input_image, target, epoch)\n",
    "\n",
    "    # saving (checkpoint) the model every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "      checkpoint_prefix = os.path.join(checkpoint_dir, f\"ckpt\") #  f\"ckpt{epoch}\"\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Time taken for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wozqyTh2wmCu"
   },
   "source": [
    "This training loop saves logs you can easily view in TensorBoard to monitor the training progress. Working locally you would launch a separate tensorboard process. In a notebook, if you want to monitor with TensorBoard it's easiest to launch the viewer before starting the training.\n",
    "\n",
    "To launch the viewer paste the following into a code-cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot22ujrlLhOd"
   },
   "outputs": [],
   "source": [
    "# #docs_infra: no_execute\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir='{LOG_DIR}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0-8Bzg22ox"
   },
   "source": [
    "Now run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "a1zZmKmvOH85",
    "outputId": "11db0de9-78b3-47d5-93da-eebbcacfad8e"
   },
   "outputs": [],
   "source": [
    "fit(train_dataset, EPOCHS, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeq9sByu86-B"
   },
   "source": [
    "If you want to share the TensorBoard results _publicly_ you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
    "\n",
    "Note: This requires a Google account.\n",
    "\n",
    "```\n",
    "!tensorboard dev upload --logdir  {log_dir}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-kT7WHRKz-E"
   },
   "source": [
    "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lGhS_LfwQoL"
   },
   "source": [
    "You can view the [results of a previous run](https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
    "\n",
    "It can also included inline using an `<iframe>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IS4c93guQ8E"
   },
   "outputs": [],
   "source": [
    "# display.IFrame(\n",
    "#     src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
    "#     width=\"100%\",\n",
    "#     height=\"1000px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMTm4peo3cem"
   },
   "source": [
    "Interpreting the logs from a GAN is more subtle than a simple classification or regression model. Things to look for::\n",
    "\n",
    "* Check that neither model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
    "* The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2: That the discriminator is on average equally uncertain about the two options.\n",
    "* For the `disc_loss` a value below `0.69` means the discriminator is doing better than random, on the combined set of real+generated images.\n",
    "* For the `gen_gan_loss` a value below `0.69` means the generator i doing better than random at foolding the descriminator.\n",
    "* As training progresses the `gen_l1_loss` should go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSSm4kfvJiqv"
   },
   "outputs": [],
   "source": [
    "!ls '{checkpoint_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RGysMU_BZhx"
   },
   "source": [
    "## Generate using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUgSnmy2nqSP"
   },
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test dataset\n",
    "for inp, tar, name in test_dataset.take(30):\n",
    "  generate_images(generator, inp, tar, name, 'test', MAX_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZn_yIr7eWb9"
   },
   "outputs": [],
   "source": [
    "# Run the trained model on all examples from the test dataset\n",
    "for inp, tar, name in test_dataset.take(len(test_dataset)):\n",
    "  generate_images(generator, inp, tar, name, 'classify', MAX_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmCIW04f8-8B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NFpiFYMTnnS"
   },
   "source": [
    "## Visualize the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRcw88SZToHR"
   },
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='{LOG_DIR}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30w5HVn_Tqvd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
