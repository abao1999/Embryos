{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsrMz8NFVvtL"
   },
   "source": [
    "## Note\n",
    "Before running this, make sure to empty the classify/ folder for this trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrwJeyrUPJi7"
   },
   "outputs": [],
   "source": [
    "# Params for trial number and checkpoint epoch to take\n",
    "TRIAL = 1\n",
    "CHECKPOINT_EPOCH = 50\n",
    "\n",
    "TRIAL_PATH = f'/content/gdrive/Shared drives/Embryo_data/pix2pix_output/{TRIAL}'\n",
    "CHECKPOINT_PATH = f'{TRIAL_PATH}/checkpoints/ckpt-{CHECKPOINT_EPOCH}'\n",
    "CHECKPOINT_DIR = f'{TRIAL_PATH}/checkpoints'\n",
    "\n",
    "CLASSIFY_PATH = f'/content/gdrive/Shared drives/Embryo_data/pix2pix_output/{TRIAL}/classify'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "nh84NELlN22m",
    "outputId": "885d7bfa-ca4e-43e7-befc-aa53df7fb1f1"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab/48919022\n",
    "import os\n",
    "from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('utils.py','wb').write(src)\n",
    "# print(os.path.abspath('utils.py'))\n",
    "import utils\n",
    "help(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7tDOhJQOB6E"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDpRjV8wOEFl",
    "outputId": "76b8ef08-017c-4d90-c65b-d82a7081ce4a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive   \n",
    "\n",
    "# mount the google drive to my Colab session\n",
    "drive.mount('/content/gdrive')\n",
    "# use the google drive in my Colab session\n",
    "home_path = '/content/gdrive/Shared drives/Embryo_data'\n",
    "print(os.listdir(home_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eIMr0oXOFLU",
    "outputId": "85378805-8fec-4548-f1b9-add049266850"
   },
   "outputs": [],
   "source": [
    "# Fixing the random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Available high quality data\n",
    "embryo_inds = utils.CLEAN_IDX\n",
    "\n",
    "# Directory of the processed *.npy files\n",
    "processed_path = f'{home_path}/processed'\n",
    "polar_processed_path = f'{processed_path}/polarization'\n",
    "\n",
    "video_time_info, train_embryos, val_embryos, test_embryos = utils.split_train_test_val(home_path, embryo_inds)\n",
    "\n",
    "z_agg_mode = \"Max Z\"\n",
    "get_data_path = lambda data_type : f'{processed_path}/{data_type}/max'\n",
    "pol_path = f'{processed_path}/polarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41DLt91eR7Jx"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 512 # 256\n",
    "IMG_HEIGHT = 512 # 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYrjHtqwONYd"
   },
   "outputs": [],
   "source": [
    "def load(bf_file):\n",
    "  # Load the bf_file as the input image\n",
    "  input_image = tf.io.read_file(bf_file)\n",
    "  input_image = tf.image.decode_png(input_image)\n",
    "\n",
    "  # Load the corresponding fluo_file as the real image\n",
    "  if type(bf_file) is str:\n",
    "    fluo_file = bf_file.replace(\"bf_data\",\"fluo_data\")\n",
    "  else:\n",
    "    fluo_file = tf.strings.regex_replace(bf_file, \"bf_data\",\"fluo_data\")\n",
    "  real_image = tf.io.read_file(fluo_file)\n",
    "  real_image = tf.image.decode_png(real_image)\n",
    "\n",
    "  # Convert both to a float32\n",
    "  input_image = tf.cast(input_image, tf.float32)\n",
    "  real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "  # # Obtain labels for embryo-time-polarization\n",
    "  # if type(bf_file) is str:\n",
    "  #     file_name = bf_file.split('/')[-1]\n",
    "  #     file_name = file_name.strip(\".png\")\n",
    "  #     labels = file_name.split('_')[1:]\n",
    "  #     labels = [int(l) for l in labels]\n",
    "  # else:\n",
    "  #     file_name = tf.strings.split(bf_file, sep=\"/\")\n",
    "  #     print(file_name.get_shape())\n",
    "  #     file_name = file_name[-1]\n",
    "  #     file_name = tf.strings.strip(\".png\")\n",
    "  #     labels = tf.strings.split(file_name, sep=\"_\")\n",
    "  #     labels = labels[1:]\n",
    "  #     labels = tf.strings.to_number(labels, out_type=tf.dtypes.int32)\n",
    "\n",
    "  return input_image, real_image, bf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "HIE3tNO5OVN-",
    "outputId": "c5a0bd99-0f9d-49db-cd0e-8f60a224473b"
   },
   "outputs": [],
   "source": [
    "BF_TRAIN_PATH = f'{processed_path}/pix2pix/train/bf_data'\n",
    "inp, re, _ = load(BF_TRAIN_PATH+'/embryo_3_0_0.png')\n",
    "# casting to int for matplotlib to show the image\n",
    "plt.figure()\n",
    "plt.imshow(tf.squeeze(inp/255.0))\n",
    "plt.figure()\n",
    "plt.imshow(tf.squeeze(re/255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfBwy-EcOX3I"
   },
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "  input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kvgrOf7OZN3"
   },
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "  cropped_image = tf.image.random_crop(\n",
    "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "\n",
    "  return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00xAAiXZOatg"
   },
   "outputs": [],
   "source": [
    "# normalizing the images to [-1, 1]\n",
    "\n",
    "def normalize(input_image, real_image):\n",
    "  input_image = (input_image / 127.5) - 1\n",
    "  real_image = (real_image / 127.5) - 1\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SooccQNbOb90"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "  # resizing to 286 x 286 x 1\n",
    "  new_size = 286 * 2\n",
    "  input_image, real_image = resize(input_image, real_image, new_size, new_size)\n",
    "\n",
    "  # randomly cropping to 512 x 512 x 1\n",
    "  input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    # random mirroring\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "    real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaDn43Y-OdGf"
   },
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "  input_image, real_image, labels = load(image_file)\n",
    "  input_image, real_image = resize(input_image, real_image,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "  input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "  return input_image, real_image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1cYKpxGO4ZO"
   },
   "outputs": [],
   "source": [
    "BF_TEST_PATH = f'{processed_path}/pix2pix/test/bf_data'\n",
    "test_dataset = tf.data.Dataset.list_files(BF_TEST_PATH+'/*.png')\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scwUVJnZO_xj"
   },
   "outputs": [],
   "source": [
    "def generate_labels(file_name):\n",
    "  # ordered output: idx, timestep, pol\n",
    "  file_name = str(file_name.numpy()[0])\n",
    "  file_name = file_name.split('/')[-1]\n",
    "  file_name = file_name.strip(\".png'\")\n",
    "  labels = file_name.split('_')[1:]\n",
    "  return [int(l) for l in labels]\n",
    "\n",
    "def generate_images(model, test_input, tar, file_name, train_test, epoch):\n",
    "  '''\n",
    "  train_test is 'train' or 'test' or 'classify'\n",
    "  '''\n",
    "\n",
    "  idx, t, pol = generate_labels(file_name)\n",
    "\n",
    "  prediction = model(test_input, training=True) \n",
    "\n",
    "  if train_test == 'classify':\n",
    "    prediction = (prediction * 0.5 + 0.5).numpy()\n",
    "    plt.imsave(f'{TRIAL_PATH}/{train_test}/{pol}/embryo{idx}_t{t}_pol{pol}.png', np.squeeze(prediction))\n",
    "    return\n",
    "\n",
    "  # turn off interactive plotting\n",
    "  # https://stackoverflow.com/questions/15713279/calling-pylab-savefig-without-display-in-ipython\n",
    "  # https://chartio.com/resources/tutorials/how-to-save-a-plot-to-a-file-using-matplotlib/\n",
    "  plt.ioff()\n",
    "  fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "  display_list = [test_input[0], tar[0], prediction[0]]\n",
    "  title = [f'Input Image (idx {idx}, t-step {t}, pol {pol})', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "  for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(np.squeeze(display_list[i] * 0.5 + 0.5))\n",
    "    plt.axis('off')\n",
    "  # plt.show()\n",
    "\n",
    "  # put epoch num first so you can order by epoch more easily\n",
    "  plt.savefig(f'{TRIAL_PATH}/{train_test}/epoch{epoch}_embryo{idx}_t{t}_pol{pol}.png', bbox_inches='tight')\n",
    "  plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRYh1fZ0PAUn",
    "outputId": "5a646f21-fd4f-46c3-b0eb-df3f8bcfbe8c"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))\n",
    "print (down_result.shape)\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)\n",
    "\n",
    "def Generator():\n",
    "  # inputs = tf.keras.layers.Input(shape=[256,256,1])\n",
    "\n",
    "  # down_stack = [\n",
    "  #   downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "  #   downsample(128, 4), # (bs, 64, 64, 128)\n",
    "  #   downsample(256, 4), # (bs, 32, 32, 256)\n",
    "  #   downsample(512, 4), # (bs, 16, 16, 512)\n",
    "  #   downsample(512, 4), # (bs, 8, 8, 512)\n",
    "  #   downsample(512, 4), # (bs, 4, 4, 512)\n",
    "  #   downsample(512, 4), # (bs, 2, 2, 512)\n",
    "  #   downsample(512, 4), # (bs, 1, 1, 512)\n",
    "  # ]\n",
    "\n",
    "  # up_stack = [\n",
    "  #   upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "  #   upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "  #   upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "  #   upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "  #   upsample(256, 4), # (bs, 32, 32, 512)\n",
    "  #   upsample(128, 4), # (bs, 64, 64, 256)\n",
    "  #   upsample(64, 4), # (bs, 128, 128, 128)\n",
    "  # ]\n",
    "\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[512,512,1])\n",
    "\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False), # (bs, 256, 256, 64)\n",
    "    downsample(128, 4), # (bs, 128, 128, 128)\n",
    "    downsample(256, 4), # (bs, 64, 64, 256)\n",
    "    downsample(512, 4), # (bs, 32, 32, 512)\n",
    "    downsample(512, 4), # (bs, 16, 16, 512)\n",
    "    downsample(512, 4), # (bs, 8, 8, 512)\n",
    "    downsample(512, 4), # (bs, 4, 4, 512)\n",
    "    downsample(512, 4), # (bs, 2, 2, 512)\n",
    "    downsample(512, 4), # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "    upsample(512, 4), # (bs, 32, 32, 1024)\n",
    "    upsample(256, 4), # (bs, 64, 64, 512)\n",
    "    upsample(128, 4), # (bs, 128, 128, 256)\n",
    "    upsample(64, 4), # (bs, 256, 256, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 512, 512, 1)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "LAMBDA = 100\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "def Discriminator():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  # inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n",
    "  # tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image')\n",
    "  inp = tf.keras.layers.Input(shape=[512, 512, 1], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[512, 512, 1], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "\n",
    "  # down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "  # down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "  # down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
    "\n",
    "  # zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "  # conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "  #                               kernel_initializer=initializer,\n",
    "  #                               use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "  # batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  # leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  # zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "  # last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "  #                               kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "  down1 = downsample(32, 4, False)(x) # (bs, 256, 256, 32)\n",
    "  down2 = downsample(64, 4)(down1) # (bs, 128, 128, 64)\n",
    "  down3 = downsample(128, 4)(down2) # (bs, 64, 64, 128)\n",
    "  down4 = downsample(256, 4)(down3) # (bs, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4) # (bs, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwhDU1aeTjtU",
    "outputId": "899b02a6-8782-4723-a661-db6c5de7ec16"
   },
   "outputs": [],
   "source": [
    "# tf.train.checkpoints_iterator(CHECKPOINT_DIR):\n",
    "checkpoint.restore(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dECOOj4PDhM"
   },
   "outputs": [],
   "source": [
    "# # Run the trained model on all examples from the test dataset\n",
    "# for inp, tar, name in test_dataset.take(len(test_dataset)):\n",
    "#   generate_images(generator, inp, tar, name, 'classify', CHECKPOINT_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBskk3YEVUYL",
    "outputId": "e4cd8c08-7483-47f6-a574-1ab570d97ca9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Pytorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI6Td0zeWcvr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxyrLnlgYeDe",
    "outputId": "565ea214-6bf7-41eb-f2c0-b1f112ce33fd"
   },
   "outputs": [],
   "source": [
    "classes = 2\n",
    "\n",
    "epochs = 15\n",
    "lr = 0.001\n",
    "per_device_batch_size = 20\n",
    "momentum = 0.9\n",
    "wd = 0.0001\n",
    "\n",
    "lr_factor = 0.75\n",
    "lr_steps = [10, 20, 30, np.inf]\n",
    "\n",
    "num_workers = 1\n",
    "num_gpus = 1\n",
    "gpus = [i for i in range(num_gpus)]\n",
    "\n",
    "device = torch.device(gpus[0]) if num_gpus > 0 else [torch.device('cpu')]\n",
    "\n",
    "batch_size = per_device_batch_size * max(num_gpus, 1)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HJZ2uSxWeAU"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeZx5TmNWiaG"
   },
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root=CLASSIFY_PATH, transform=transform, target_transform=None, is_valid_file=check_image)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, sampler=None, num_workers=num_workers, drop_last=True)\n",
    "classes = ('unpolarized', 'polarized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruOBiVVfXYXV"
   },
   "outputs": [],
   "source": [
    "# Step 3: define the PyTorch architecture to load in the weights\n",
    "\n",
    "class SimpleConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvolutionalNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 20, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(50, 100, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # # self.dropout = nn.Dropout(p=0.2)\n",
    "        # self.batchnorm1 = nn.BatchNorm2d(20)\n",
    "        # self.batchnorm2 = nn.BatchNorm2d(50)\n",
    "        \n",
    "        # cf comments in forward() to have step by step comments on shape progression\n",
    "        # self.fc1 = nn.Linear(256 * 64 * 64, 64)\n",
    "        self.fc1 = nn.Linear(100 * 64 * 64, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass,\n",
    "        x shape is (batch_size, 3, 512, 512)\n",
    "        (color channel first)\n",
    "        in the comments, we omit the batch_size in the shape\n",
    "        \"\"\"\n",
    "        # print(x.shape)\n",
    "\n",
    "        # shape : 3x512x512 -> 20x512x512\n",
    "        x = F.dropout(F.relu(self.conv1(x)), p=0.2, training=True)\n",
    "        # x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 20x512x512 -> 20x256x256\n",
    "        x = self.pool1(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 20x256x256 -> 50x256x256\n",
    "        x = F.dropout(F.relu(self.conv2(x)), p=0.2, training=True)\n",
    "        # x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        # x = F.relu(self.conv2(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 50x256x256 -> 50x128x128\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 50x128x128 -> 100x128x128\n",
    "        x = F.dropout(F.relu(self.conv3(x)), p=0.2, training=True)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 100x128x128 -> 100x64x64\n",
    "        x = self.pool3(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 100x64x64 -> .\n",
    "        x = x.view(-1, 100 * 64 *64)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # . -> 64\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 64 -> 2\n",
    "        # The softmax non-linearity is applied later (cf createLossAndOptimizer() fn)\n",
    "        x = self.fc2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQRRaEzOXY82"
   },
   "outputs": [],
   "source": [
    "# Step 4: instantiate the architecture with saved params\n",
    "\n",
    "network = SimpleConvolutionalNetwork()\n",
    "network = network.to(device)\n",
    "name = 'torchCNN-max-windowNone-windowNone-batch20-lr0.0001-mom0.9-epoch2'\n",
    "network.load_state_dict(torch.load(f'/content/gdrive/Shared drives/Embryo_data/models/best/{name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oojUgN2Xb_g"
   },
   "outputs": [],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zlKgnCxXePn"
   },
   "outputs": [],
   "source": [
    "def dataset_accuracy(net, data_loader, name=\"\", print_first_k_mistakes=0):\n",
    "    net = net.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    to_print = []\n",
    "    for images_loader, labels_loader in data_loader:\n",
    "        images, labels = images_loader.to(device), labels_loader.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if print_first_k_mistakes > 0:\n",
    "            for idx, match in enumerate(zip(predicted, labels)):\n",
    "                if match[0] != match[1]:\n",
    "                    error = \"Actually unpolarized:\"\n",
    "                    if match[1] == 1:\n",
    "                        error = \"Actually polarized:\"\n",
    "                    to_print.append((error, images_loader[idx]))\n",
    "                    print_first_k_mistakes -= 1\n",
    "    accuracy = 100 * float(correct) / total\n",
    "    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n",
    "\n",
    "    if len(to_print) > 0:\n",
    "        print('\\n'.join('{:>10}'.format(label[0]) for label in to_print))\n",
    "        imshow(torchvision.utils.make_grid([img[1] for img in to_print]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9CT8TaJYE7x"
   },
   "outputs": [],
   "source": [
    "def test_set_accuracy(net):\n",
    "    dataset_accuracy(net, test_loader, \"test\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwwo0zQ7YHMa"
   },
   "outputs": [],
   "source": [
    "test_set_accuracy(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTNI9HOuYK0P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WexjPOGsZcq2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
