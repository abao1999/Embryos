{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of classify_fluo_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f839397f384424bb6dc5ae34a6899e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_010af741b5e64fa28167787c8d73dc3d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6510b2be19314c6b976ce4325d63bf9e",
              "IPY_MODEL_edd2321858c5419b92331134606fc061"
            ]
          }
        },
        "010af741b5e64fa28167787c8d73dc3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6510b2be19314c6b976ce4325d63bf9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9544b29df57b4c5984147ea6619a991a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a64b86b019264d77a428fddc762d2ae9"
          }
        },
        "edd2321858c5419b92331134606fc061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_400f503ffc6546e7b720866f76b619a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:19&lt;00:00, 5.15MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5440bd743bf4fadb50fd2c351147a9e"
          }
        },
        "9544b29df57b4c5984147ea6619a991a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a64b86b019264d77a428fddc762d2ae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "400f503ffc6546e7b720866f76b619a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5440bd743bf4fadb50fd2c351147a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H29TGLErCP5r"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG53T9hI2nlC"
      },
      "source": [
        "import numpy as np\n",
        "import os, time, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jUjUw-DAlXlZ",
        "outputId": "f594ca81-df9e-4f1b-8607-1dc03ce0ab1f"
      },
      "source": [
        "# https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab/48919022\n",
        "import os\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('utils.py','wb').write(src)\n",
        "import utils\n",
        "help(utils)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f16687a1-1cec-46b0-8a31-190807bd5ec9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f16687a1-1cec-46b0-8a31-190807bd5ec9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n",
            "Help on module utils:\n",
            "\n",
            "NAME\n",
            "    utils\n",
            "\n",
            "FUNCTIONS\n",
            "    avg_across_z(img, normalize=False)\n",
            "        Returns a new image where each pixel\n",
            "        intensity is the average for that pixel across\n",
            "        all images in the z-stack.\n",
            "    \n",
            "    get_img_at_t(t, img)\n",
            "    \n",
            "    get_max_pixel(embryos, data_path)\n",
            "        Obtains the maximum pixel value across a set of embryos\n",
            "        embryos: subset of p_embryo... train, val, test\n",
            "        data_path: path from which to load processed np embryo data\n",
            "    \n",
            "    get_z_slice(z, img)\n",
            "    \n",
            "    max_across_z(img, normalize=False)\n",
            "        Returns a new image where each pixel\n",
            "        intensity is the maximum for that pixel across\n",
            "        all images in the z-stack.\n",
            "    \n",
            "    middle_z(img)\n",
            "    \n",
            "    min_across_z(img, normalize=False)\n",
            "        Returns a new image where each pixel\n",
            "        intensity is the minimum for that pixel across\n",
            "        all images in the z-stack.\n",
            "    \n",
            "    normalize(img)\n",
            "        Normalizes pixel values across all images in img\n",
            "        to range 0-1.\n",
            "    \n",
            "    save_nps_as_png(embryos, save_path, specs, window=None, normalize='per_embryo', dim=2, pol_subdir=True)\n",
            "        Save dataset in image format, sorted by polarization state\n",
            "        embryos: subset of p_embryo... train, val, test\n",
            "        specs = (data_path, pol_path, video_time_info)\n",
            "        save_path: path to save png to... data_path + {'train', 'val', 'test'}\n",
            "        specs: tuple with data_path (str: processed np embryo data),\n",
            "                          pol_path (str: embryo polarization labels),\n",
            "                          video_time_info (df: info per embryo)\n",
            "        window: number of t steps from first polarized index to ignore\n",
            "        normalize: type of normalization to apply (per_embryo, per_timestep, #)\n",
            "        dim: 2 or 3 (using 2d representation of z-stack or 3d selection of slices)\n",
            "        pol_subdir: whether to save images to \"{save_path}/{pol}/\" or \"{save_path}/\"\n",
            "    \n",
            "    save_nps_subset_z(embryos, save_path, specs, window=None, normalize='per_embryo', mode='middle', n_slices=3, pol_subdir=True)\n",
            "        Save dataset in image format, sorted by polarization state\n",
            "        embryos: subset of p_embryo... train, val, test\n",
            "        specs = (data_path, pol_path, video_time_info)\n",
            "        save_path: path to save png to... data_path + {'train', 'val', 'test'}\n",
            "        specs: tuple with data_path (str: processed np embryo data),\n",
            "                          pol_path (str: embryo polarization labels),\n",
            "                          video_time_info (df: info per embryo)\n",
            "        window: number of t steps from first polarized index to ignore\n",
            "        normalize: type of normalization to apply (per_embryo, per_timestep, #)\n",
            "        mode: min, middle, max... where to start z slices (if middle, get z slices around middle)\n",
            "        n_slices: number of z slices to get\n",
            "        pol_subdir: whether to save images to \"{save_path}/{pol}/\" or \"{save_path}/\"\n",
            "    \n",
            "    split_train_test_val(home_path, embryo_inds)\n",
            "        Splits a set of embryos into train/test/val:\n",
            "        home_path: path to shared drive folder\n",
            "        embryo_inds: list of embryos indices to use\n",
            "    \n",
            "    within_window(embryo_idx, t, window, video_time_info)\n",
            "        Returns if an embryo timestep is close to the 1st polarized time\n",
            "        embryo_idx: index of current embryo (value from 'embryo_index' col)\n",
            "        t: timestep of current\n",
            "        window: number of t steps from first polarized index to ignore\n",
            "        video_time_info: dataframe with labeling/timing information per embryo\n",
            "\n",
            "DATA\n",
            "    CLEAN_IDX = [3, 12, 13, 16, 18, 19, 23, 24, 25, 33, 39, 42, 46, 47, 76...\n",
            "    CLEAN_IDX_FULL = [3, 12, 13, 16, 18, 19, 23, 24, 25, 33, 39, 42, 46, 4...\n",
            "    FULL_IDX = [1, 3, 12, 13, 16, 18, 19, 24, 39, 40, 42, 45, 46, 47, 49, ...\n",
            "    PARTIAL_IDX = [1, 3, 12, 13, 16, 18, 19, 24, 39, 40, 42, 45, 46, 47, 4...\n",
            "\n",
            "FILE\n",
            "    /content/utils.py\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGh49iIJCP5u"
      },
      "source": [
        "# 4. Transfer Learning with Your Own Image Dataset\n",
        "\n",
        "Dataset size is a big factor in the performance of deep learning models.\n",
        "``ImageNet`` has over one million labeled images, but\n",
        "we often don't have so much labeled data in other domains.\n",
        "Training a deep learning models on small datasets may lead to severe overfitting.\n",
        "\n",
        "Transfer learning is a technique that addresses this problem.\n",
        "The idea is simple: we can start training with a pre-trained model,\n",
        "instead of starting from scratch.\n",
        "As Isaac Newton said, \"If I have seen further it is by standing on the\n",
        "shoulders of Giants\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCMW-mPvwrcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45359448-9eea-4d73-d59a-9b072ab71148"
      },
      "source": [
        "import os\n",
        "from google.colab import files, drive   \n",
        "import pandas as pd\n",
        "\n",
        "# mount the google drive to my Colab session\n",
        "drive.mount('/content/gdrive')\n",
        "# use the google drive in my Colab session\n",
        "print(os.listdir('/content/gdrive/Shared drives/Embryo_data'))\n",
        "\n",
        "home_path = '/content/gdrive/Shared drives/Embryo_data'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "['mxnet_cnn2d_embryo_58_fine_tune_data_aug_ResNet50_v2_order_random.ipynb', 'Embryo3', 'Embryo12', 'Embryo13', 'Embryo16', 'Embryo19', 'Embryo18', 'Embryo24', 'Embryo39', 'Embryo42', 'Embryo46', 'Embryo47', 'Embryo23', 'Embryo33', 'Embryo25', 'Embryo95', 'Embryo97', 'Embryo96', 'Embryo98', 'Embryo101', 'Embryo99', 'Embryo100', 'Embryo102', 'Embryo76', 'Embryo78', 'Embryo81', 'Embryo79', 'Embryo80', 'Embryo84', 'Embryo85', 'Embryo87', 'Embryo88', 'Embryo92', 'Embryo94', 'Embryo93', 'embryo_info_CS101.xlsx', 'data', 'processed', 'models', 'pix2pix_PyTorch-GAN.ipynb', 'annotation.xlsx', 'Embryo110', 'Embryo109', 'Embryo111', 'Embryo113', 'Embryo112', 'Embryo114', 'Embryo116', 'Embryo115', 'Embryo117', 'Embryo118', 'Embryo119', 'Embryo120', 'Embryo103', 'Embryo104', 'Embryo105', 'Embryo107', 'Embryo106', 'Embryo108', 'pix2pix_output', 'images']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhHeN0l4CP5y"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "First, let's import all other necessary libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiAE4g8aCP53"
      },
      "source": [
        "We set the hyperparameters as following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQCpNXYCP53"
      },
      "source": [
        "classes = 2\n",
        "\n",
        "epochs = 60\n",
        "per_device_batch_size = 16\n",
        "\n",
        "num_gpus = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
        "batch_size = per_device_batch_size * max(num_gpus, 1)\n",
        "log_interval = 100\n",
        "step = 1\n",
        "lr = 0.1\n",
        "gamma = 0.7"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THLMmIhfCP56"
      },
      "source": [
        "Things to keep in mind:\n",
        "\n",
        "1. ``epochs = 5`` is just for this tutorial with the tiny dataset. please change it to a larger number in your experiments, for instance 40.\n",
        "2. ``per_device_batch_size`` is also set to a small number. In your experiments you can try larger number like 64.\n",
        "3. remember to tune ``num_gpus`` and ``num_workers`` according to your machine.\n",
        "4. A pre-trained model is already in a pretty good status. So we can start with a small ``lr``.\n",
        "\n",
        "## Data Augmentation\n",
        "\n",
        "In transfer learning, data augmentation can also help.\n",
        "We use the following augmentation in training:\n",
        "\n",
        "2. Randomly crop the image and resize it to 224x224\n",
        "3. Randomly flip the image horizontally\n",
        "4. Randomly jitter color and add noise\n",
        "5. Transpose the data from height*width*num_channels to num_channels*height*width, and map values from [0, 255] to [0, 1]\n",
        "6. Normalize with the mean and standard deviation from the ImageNet dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgiHxUECP56"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OKZ2hq1CP59"
      },
      "source": [
        "With the data augmentation functions, we can define our data loaders:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5-MROpDj8XW"
      },
      "source": [
        "class ImageFolderDataset(Dataset):\n",
        "    \"\"\"A dataset for loading image files stored in a folder structure.\n",
        "\n",
        "    like::\n",
        "\n",
        "        root/car/0001.jpg\n",
        "        root/car/xxxa.jpg\n",
        "        root/car/yyyb.jpg\n",
        "        root/bus/123.jpg\n",
        "        root/bus/023.jpg\n",
        "        root/bus/wwww.jpg\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root : str\n",
        "        Path to root directory.\n",
        "    flag : {0, 1}, default 1\n",
        "        If 0, always convert loaded images to greyscale (1 channel).\n",
        "        If 1, always convert loaded images to colored (3 channels).\n",
        "    transform : callable, default None\n",
        "        A function that takes data and label and transforms them::\n",
        "\n",
        "            transform = lambda data, label: (data.astype(np.float32)/255, label)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    synsets : list\n",
        "        List of class names. `synsets[i]` is the name for the integer label `i`\n",
        "    items : list of tuples\n",
        "        List of all images in (filename, label) pairs.\n",
        "    \"\"\"\n",
        "    def __init__(self, root, flag=1, transform=None):\n",
        "        self._root = os.path.expanduser(root)\n",
        "        self._flag = flag\n",
        "        self._transform = transform\n",
        "        self._exts = ['.jpg', '.jpeg', '.png']\n",
        "        self._list_images(self._root)\n",
        "\n",
        "    def _list_images(self, root):\n",
        "        self.items = []\n",
        "\n",
        "        for filename in sorted(os.listdir(root)):\n",
        "          filepath = os.path.join(root, filename)\n",
        "          self.items.append(filepath)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ind = self.items[idx].find('embryo')\n",
        "        [embryo_num, embryo_t, label] = [int(num) for num in self.items[idx][ind:-4].split('_')[1:]]\n",
        "        img = Image.open(self.items[idx]).convert('RGB')\n",
        "        if self._transform is not None:\n",
        "            img = self._transform(img)\n",
        "        return {'img': img, 'label': label, 'embryo_num': embryo_num, 'embryo_t': embryo_t}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226lVHP2CP5-"
      },
      "source": [
        "embryo_type = 'bf'\n",
        "processed_path = f'{home_path}/processed'\n",
        "train_path = f'{processed_path}/pix2pix/train/{embryo_type}_data'\n",
        "val_path = f'{processed_path}/pix2pix/val/{embryo_type}_data'\n",
        "test_path = f'{processed_path}/pix2pix/test/{embryo_type}_data'\n",
        "\n",
        "train_dataset = ImageFolderDataset(train_path, transform=transform_train)\n",
        "val_dataset = ImageFolderDataset(val_path, transform=transform_train)\n",
        "test_dataset = ImageFolderDataset(test_path, transform=transform_test)\n",
        "\n",
        "train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "val_data = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_data = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IkfDxPxCP6B"
      },
      "source": [
        "Note that only ``train_data`` uses ``transform_train``, while\n",
        "``val_data`` and ``test_data`` use ``transform_test`` to produce deterministic\n",
        "results for evaluation.\n",
        "\n",
        "## Model and Trainer\n",
        "\n",
        "We use a pre-trained ``ResNet50_v2`` model, which has balanced accuracy and\n",
        "computation cost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8KZlje8CP6B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "1f839397f384424bb6dc5ae34a6899e8",
            "010af741b5e64fa28167787c8d73dc3d",
            "6510b2be19314c6b976ce4325d63bf9e",
            "edd2321858c5419b92331134606fc061",
            "9544b29df57b4c5984147ea6619a991a",
            "a64b86b019264d77a428fddc762d2ae9",
            "400f503ffc6546e7b720866f76b619a3",
            "d5440bd743bf4fadb50fd2c351147a9e"
          ]
        },
        "outputId": "c3eb18ad-aeb9-4d26-fa89-0dc34f7b5558"
      },
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, classes)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
        "scheduler = StepLR(optimizer, step_size=step, gamma=gamma)\n",
        "\n",
        "# Path to save trained models to, create folder\n",
        "model_name = 'ResNet50_v2'\n",
        "model_save_path = os.path.join(home_path, 'models', f'{model_name}-{embryo_type}-batch{per_device_batch_size}-lr{lr}-step{step}-gamma{gamma}')\n",
        "if not os.path.isdir(model_save_path):\n",
        "  os.mkdir(model_save_path)\n",
        "else:\n",
        "  print(model_save_path, 'already exists')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f839397f384424bb6dc5ae34a6899e8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjIqPfnNCP6E"
      },
      "source": [
        "Here's an illustration of the pre-trained model\n",
        "and our newly defined model:\n",
        "\n",
        "|image-model|\n",
        "\n",
        "Specifically, we define the new model by::\n",
        "\n",
        "1. load the pre-trained model\n",
        "2. re-define the output layer for the new task\n",
        "3. train the network\n",
        "\n",
        "This is called \"fine-tuning\", i.e. we have a model trained on another task,\n",
        "and we would like to tune it for the dataset we have in hand.\n",
        "\n",
        "We define a evaluation function for validation and testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJG2Rg_NCP6G"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Following is the main training loop. It is the same as the loop in\n",
        "`CIFAR10 <dive_deep_cifar10.html>`__\n",
        "and ImageNet.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Once again, in order to go through the tutorial faster, we are training on a small\n",
        "    subset of the original ``MINC-2500`` dataset, and for only 5 epochs. By training on the\n",
        "    full dataset with 40 epochs, it is expected to get accuracy around 80% on test data.</p></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LKYIONQY0eO"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    '''\n",
        "    This is your training function. When you call this function, the model is\n",
        "    trained for 1 epoch.\n",
        "    '''\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()# Set the model to training mode\n",
        "    losses = []\n",
        "    for batch_idx, data0 in enumerate(train_loader):\n",
        "        data = data0['img']\n",
        "        target = data0['label']\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()               # Clear the gradient\n",
        "        output = model(data)                # Make predictions\n",
        "        loss = criterion(output, target)   # Compute loss\n",
        "        loss.backward()                     # Gradient computation\n",
        "        optimizer.step()                    # Perform a single optimization step\n",
        "        losses.append(loss.item())\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_dataset),\n",
        "                100. * batch_idx / len(train_dataset), losses[-1]))\n",
        "    return np.mean(losses)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYsL0aY5ZK6j"
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "    model.eval()    # Set the model to inference mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():   # For the inference step, gradient is not computed\n",
        "        for data0 in test_loader:\n",
        "            data = data0['img']\n",
        "            target = data0['label']\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += len(target)\n",
        "\n",
        "    test_loss /= total\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, total,\n",
        "        100. * correct / total))\n",
        "    return test_loss, correct / total"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwRaiiomCP6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c3ca2f-ee17-445c-afb5-a48de86f8aa5"
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "start = time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(model, device, train_data, optimizer, epoch)\n",
        "    test_loss = test(model, device, val_data)\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    scheduler.step()    # learning rate scheduler\n",
        "    torch.save(model.state_dict(), f\"{model_save_path}/epoch{epoch}.pt\")\n",
        "print('Train Time:', time.time()-start)\n",
        "np.save(f\"{model_save_path}/train_loss.npy\", np.array(train_losses))\n",
        "np.save(f\"{model_save_path}/test_loss.npy\", np.array(test_losses))\n",
        "\n",
        "print(\"\\nFinal Performance!\")\n",
        "print(\"Training Set:\")\n",
        "test(model, device, train_data)\n",
        "print(\"Validation Set:\")\n",
        "test(model, device, val_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1900 (0%)]\tLoss: 0.719597\n",
            "Train Epoch: 1 [1600/1900 (5%)]\tLoss: 0.172699\n",
            "\n",
            "Test set: Average loss: 2.5152, Accuracy: 102/312 (33%)\n",
            "\n",
            "Train Epoch: 2 [0/1900 (0%)]\tLoss: 0.061003\n",
            "Train Epoch: 2 [1600/1900 (5%)]\tLoss: 0.075261\n",
            "\n",
            "Test set: Average loss: 0.2657, Accuracy: 264/312 (85%)\n",
            "\n",
            "Train Epoch: 3 [0/1900 (0%)]\tLoss: 0.035908\n",
            "Train Epoch: 3 [1600/1900 (5%)]\tLoss: 0.020962\n",
            "\n",
            "Test set: Average loss: 0.3232, Accuracy: 280/312 (90%)\n",
            "\n",
            "Train Epoch: 4 [0/1900 (0%)]\tLoss: 0.012616\n",
            "Train Epoch: 4 [1600/1900 (5%)]\tLoss: 0.008348\n",
            "\n",
            "Test set: Average loss: 0.1677, Accuracy: 285/312 (91%)\n",
            "\n",
            "Train Epoch: 5 [0/1900 (0%)]\tLoss: 0.006018\n",
            "Train Epoch: 5 [1600/1900 (5%)]\tLoss: 0.183309\n",
            "\n",
            "Test set: Average loss: 0.2425, Accuracy: 280/312 (90%)\n",
            "\n",
            "Train Epoch: 6 [0/1900 (0%)]\tLoss: 0.001409\n",
            "Train Epoch: 6 [1600/1900 (5%)]\tLoss: 0.000935\n",
            "\n",
            "Test set: Average loss: 0.1856, Accuracy: 290/312 (93%)\n",
            "\n",
            "Train Epoch: 7 [0/1900 (0%)]\tLoss: 0.013225\n",
            "Train Epoch: 7 [1600/1900 (5%)]\tLoss: 0.003256\n",
            "\n",
            "Test set: Average loss: 0.2018, Accuracy: 291/312 (93%)\n",
            "\n",
            "Train Epoch: 8 [0/1900 (0%)]\tLoss: 0.011647\n",
            "Train Epoch: 8 [1600/1900 (5%)]\tLoss: 0.000683\n",
            "\n",
            "Test set: Average loss: 0.1717, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 9 [0/1900 (0%)]\tLoss: 0.003219\n",
            "Train Epoch: 9 [1600/1900 (5%)]\tLoss: 0.001532\n",
            "\n",
            "Test set: Average loss: 0.1682, Accuracy: 291/312 (93%)\n",
            "\n",
            "Train Epoch: 10 [0/1900 (0%)]\tLoss: 0.002501\n",
            "Train Epoch: 10 [1600/1900 (5%)]\tLoss: 0.012885\n",
            "\n",
            "Test set: Average loss: 0.1941, Accuracy: 287/312 (92%)\n",
            "\n",
            "Train Epoch: 11 [0/1900 (0%)]\tLoss: 0.003052\n",
            "Train Epoch: 11 [1600/1900 (5%)]\tLoss: 0.001625\n",
            "\n",
            "Test set: Average loss: 0.1828, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 12 [0/1900 (0%)]\tLoss: 0.029062\n",
            "Train Epoch: 12 [1600/1900 (5%)]\tLoss: 0.001304\n",
            "\n",
            "Test set: Average loss: 0.1861, Accuracy: 287/312 (92%)\n",
            "\n",
            "Train Epoch: 13 [0/1900 (0%)]\tLoss: 0.003561\n",
            "Train Epoch: 13 [1600/1900 (5%)]\tLoss: 0.013597\n",
            "\n",
            "Test set: Average loss: 0.2080, Accuracy: 290/312 (93%)\n",
            "\n",
            "Train Epoch: 14 [0/1900 (0%)]\tLoss: 0.000920\n",
            "Train Epoch: 14 [1600/1900 (5%)]\tLoss: 0.002722\n",
            "\n",
            "Test set: Average loss: 0.1650, Accuracy: 295/312 (95%)\n",
            "\n",
            "Train Epoch: 15 [0/1900 (0%)]\tLoss: 0.045430\n",
            "Train Epoch: 15 [1600/1900 (5%)]\tLoss: 0.001059\n",
            "\n",
            "Test set: Average loss: 0.1615, Accuracy: 293/312 (94%)\n",
            "\n",
            "Train Epoch: 16 [0/1900 (0%)]\tLoss: 0.003566\n",
            "Train Epoch: 16 [1600/1900 (5%)]\tLoss: 0.002618\n",
            "\n",
            "Test set: Average loss: 0.1902, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 17 [0/1900 (0%)]\tLoss: 0.000844\n",
            "Train Epoch: 17 [1600/1900 (5%)]\tLoss: 0.000733\n",
            "\n",
            "Test set: Average loss: 0.2025, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 18 [0/1900 (0%)]\tLoss: 0.000576\n",
            "Train Epoch: 18 [1600/1900 (5%)]\tLoss: 0.002484\n",
            "\n",
            "Test set: Average loss: 0.2052, Accuracy: 285/312 (91%)\n",
            "\n",
            "Train Epoch: 19 [0/1900 (0%)]\tLoss: 0.002365\n",
            "Train Epoch: 19 [1600/1900 (5%)]\tLoss: 0.001399\n",
            "\n",
            "Test set: Average loss: 0.1734, Accuracy: 292/312 (94%)\n",
            "\n",
            "Train Epoch: 20 [0/1900 (0%)]\tLoss: 0.002773\n",
            "Train Epoch: 20 [1600/1900 (5%)]\tLoss: 0.001277\n",
            "\n",
            "Test set: Average loss: 0.1811, Accuracy: 292/312 (94%)\n",
            "\n",
            "Train Epoch: 21 [0/1900 (0%)]\tLoss: 0.009688\n",
            "Train Epoch: 21 [1600/1900 (5%)]\tLoss: 0.089554\n",
            "\n",
            "Test set: Average loss: 0.1849, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 22 [0/1900 (0%)]\tLoss: 0.001425\n",
            "Train Epoch: 22 [1600/1900 (5%)]\tLoss: 0.000390\n",
            "\n",
            "Test set: Average loss: 0.1932, Accuracy: 290/312 (93%)\n",
            "\n",
            "Train Epoch: 23 [0/1900 (0%)]\tLoss: 0.001799\n",
            "Train Epoch: 23 [1600/1900 (5%)]\tLoss: 0.000583\n",
            "\n",
            "Test set: Average loss: 0.1650, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 24 [0/1900 (0%)]\tLoss: 0.001977\n",
            "Train Epoch: 24 [1600/1900 (5%)]\tLoss: 0.001781\n",
            "\n",
            "Test set: Average loss: 0.1891, Accuracy: 293/312 (94%)\n",
            "\n",
            "Train Epoch: 25 [0/1900 (0%)]\tLoss: 0.000989\n",
            "Train Epoch: 25 [1600/1900 (5%)]\tLoss: 0.005876\n",
            "\n",
            "Test set: Average loss: 0.1732, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 26 [0/1900 (0%)]\tLoss: 0.000240\n",
            "Train Epoch: 26 [1600/1900 (5%)]\tLoss: 0.000720\n",
            "\n",
            "Test set: Average loss: 0.2258, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 27 [0/1900 (0%)]\tLoss: 0.000794\n",
            "Train Epoch: 27 [1600/1900 (5%)]\tLoss: 0.000681\n",
            "\n",
            "Test set: Average loss: 0.1813, Accuracy: 291/312 (93%)\n",
            "\n",
            "Train Epoch: 28 [0/1900 (0%)]\tLoss: 0.000824\n",
            "Train Epoch: 28 [1600/1900 (5%)]\tLoss: 0.002953\n",
            "\n",
            "Test set: Average loss: 0.2014, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 29 [0/1900 (0%)]\tLoss: 0.000571\n",
            "Train Epoch: 29 [1600/1900 (5%)]\tLoss: 0.021832\n",
            "\n",
            "Test set: Average loss: 0.2302, Accuracy: 283/312 (91%)\n",
            "\n",
            "Train Epoch: 30 [0/1900 (0%)]\tLoss: 0.000823\n",
            "Train Epoch: 30 [1600/1900 (5%)]\tLoss: 0.001978\n",
            "\n",
            "Test set: Average loss: 0.1819, Accuracy: 290/312 (93%)\n",
            "\n",
            "Train Epoch: 31 [0/1900 (0%)]\tLoss: 0.000632\n",
            "Train Epoch: 31 [1600/1900 (5%)]\tLoss: 0.000919\n",
            "\n",
            "Test set: Average loss: 0.1732, Accuracy: 291/312 (93%)\n",
            "\n",
            "Train Epoch: 32 [0/1900 (0%)]\tLoss: 0.002144\n",
            "Train Epoch: 32 [1600/1900 (5%)]\tLoss: 0.003330\n",
            "\n",
            "Test set: Average loss: 0.1954, Accuracy: 287/312 (92%)\n",
            "\n",
            "Train Epoch: 33 [0/1900 (0%)]\tLoss: 0.017283\n",
            "Train Epoch: 33 [1600/1900 (5%)]\tLoss: 0.004815\n",
            "\n",
            "Test set: Average loss: 0.1775, Accuracy: 292/312 (94%)\n",
            "\n",
            "Train Epoch: 34 [0/1900 (0%)]\tLoss: 0.010298\n",
            "Train Epoch: 34 [1600/1900 (5%)]\tLoss: 0.001144\n",
            "\n",
            "Test set: Average loss: 0.1798, Accuracy: 290/312 (93%)\n",
            "\n",
            "Train Epoch: 35 [0/1900 (0%)]\tLoss: 0.005688\n",
            "Train Epoch: 35 [1600/1900 (5%)]\tLoss: 0.000680\n",
            "\n",
            "Test set: Average loss: 0.2067, Accuracy: 294/312 (94%)\n",
            "\n",
            "Train Epoch: 36 [0/1900 (0%)]\tLoss: 0.015281\n",
            "Train Epoch: 36 [1600/1900 (5%)]\tLoss: 0.011089\n",
            "\n",
            "Test set: Average loss: 0.2106, Accuracy: 287/312 (92%)\n",
            "\n",
            "Train Epoch: 37 [0/1900 (0%)]\tLoss: 0.000942\n",
            "Train Epoch: 37 [1600/1900 (5%)]\tLoss: 0.000751\n",
            "\n",
            "Test set: Average loss: 0.1395, Accuracy: 293/312 (94%)\n",
            "\n",
            "Train Epoch: 38 [0/1900 (0%)]\tLoss: 0.001525\n",
            "Train Epoch: 38 [1600/1900 (5%)]\tLoss: 0.003141\n",
            "\n",
            "Test set: Average loss: 0.1882, Accuracy: 285/312 (91%)\n",
            "\n",
            "Train Epoch: 39 [0/1900 (0%)]\tLoss: 0.001406\n",
            "Train Epoch: 39 [1600/1900 (5%)]\tLoss: 0.001396\n",
            "\n",
            "Test set: Average loss: 0.1896, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 40 [0/1900 (0%)]\tLoss: 0.001686\n",
            "Train Epoch: 40 [1600/1900 (5%)]\tLoss: 0.001029\n",
            "\n",
            "Test set: Average loss: 0.1535, Accuracy: 290/312 (93%)\n",
            "\n",
            "Train Epoch: 41 [0/1900 (0%)]\tLoss: 0.001199\n",
            "Train Epoch: 41 [1600/1900 (5%)]\tLoss: 0.001225\n",
            "\n",
            "Test set: Average loss: 0.2048, Accuracy: 293/312 (94%)\n",
            "\n",
            "Train Epoch: 42 [0/1900 (0%)]\tLoss: 0.000807\n",
            "Train Epoch: 42 [1600/1900 (5%)]\tLoss: 0.000447\n",
            "\n",
            "Test set: Average loss: 0.1934, Accuracy: 287/312 (92%)\n",
            "\n",
            "Train Epoch: 43 [0/1900 (0%)]\tLoss: 0.001942\n",
            "Train Epoch: 43 [1600/1900 (5%)]\tLoss: 0.000692\n",
            "\n",
            "Test set: Average loss: 0.1678, Accuracy: 293/312 (94%)\n",
            "\n",
            "Train Epoch: 44 [0/1900 (0%)]\tLoss: 0.032657\n",
            "Train Epoch: 44 [1600/1900 (5%)]\tLoss: 0.001540\n",
            "\n",
            "Test set: Average loss: 0.1501, Accuracy: 295/312 (95%)\n",
            "\n",
            "Train Epoch: 45 [0/1900 (0%)]\tLoss: 0.086832\n",
            "Train Epoch: 45 [1600/1900 (5%)]\tLoss: 0.007000\n",
            "\n",
            "Test set: Average loss: 0.2227, Accuracy: 287/312 (92%)\n",
            "\n",
            "Train Epoch: 46 [0/1900 (0%)]\tLoss: 0.000662\n",
            "Train Epoch: 46 [1600/1900 (5%)]\tLoss: 0.013084\n",
            "\n",
            "Test set: Average loss: 0.1819, Accuracy: 286/312 (92%)\n",
            "\n",
            "Train Epoch: 47 [0/1900 (0%)]\tLoss: 0.010560\n",
            "Train Epoch: 47 [1600/1900 (5%)]\tLoss: 0.004399\n",
            "\n",
            "Test set: Average loss: 0.1876, Accuracy: 289/312 (93%)\n",
            "\n",
            "Train Epoch: 48 [0/1900 (0%)]\tLoss: 0.007670\n",
            "Train Epoch: 48 [1600/1900 (5%)]\tLoss: 0.001739\n",
            "\n",
            "Test set: Average loss: 0.1994, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 49 [0/1900 (0%)]\tLoss: 0.002825\n",
            "Train Epoch: 49 [1600/1900 (5%)]\tLoss: 0.001935\n",
            "\n",
            "Test set: Average loss: 0.1714, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 50 [0/1900 (0%)]\tLoss: 0.002082\n",
            "Train Epoch: 50 [1600/1900 (5%)]\tLoss: 0.005157\n",
            "\n",
            "Test set: Average loss: 0.1612, Accuracy: 291/312 (93%)\n",
            "\n",
            "Train Epoch: 51 [0/1900 (0%)]\tLoss: 0.002021\n",
            "Train Epoch: 51 [1600/1900 (5%)]\tLoss: 0.001485\n",
            "\n",
            "Test set: Average loss: 0.1861, Accuracy: 293/312 (94%)\n",
            "\n",
            "Train Epoch: 52 [0/1900 (0%)]\tLoss: 0.001094\n",
            "Train Epoch: 52 [1600/1900 (5%)]\tLoss: 0.003538\n",
            "\n",
            "Test set: Average loss: 0.1697, Accuracy: 290/312 (93%)\n",
            "\n",
            "Train Epoch: 53 [0/1900 (0%)]\tLoss: 0.001282\n",
            "Train Epoch: 53 [1600/1900 (5%)]\tLoss: 0.000952\n",
            "\n",
            "Test set: Average loss: 0.2168, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 54 [0/1900 (0%)]\tLoss: 0.007371\n",
            "Train Epoch: 54 [1600/1900 (5%)]\tLoss: 0.005660\n",
            "\n",
            "Test set: Average loss: 0.1885, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 55 [0/1900 (0%)]\tLoss: 0.001302\n",
            "Train Epoch: 55 [1600/1900 (5%)]\tLoss: 0.006832\n",
            "\n",
            "Test set: Average loss: 0.1712, Accuracy: 291/312 (93%)\n",
            "\n",
            "Train Epoch: 56 [0/1900 (0%)]\tLoss: 0.004162\n",
            "Train Epoch: 56 [1600/1900 (5%)]\tLoss: 0.002556\n",
            "\n",
            "Test set: Average loss: 0.2042, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 57 [0/1900 (0%)]\tLoss: 0.004678\n",
            "Train Epoch: 57 [1600/1900 (5%)]\tLoss: 0.000588\n",
            "\n",
            "Test set: Average loss: 0.2144, Accuracy: 285/312 (91%)\n",
            "\n",
            "Train Epoch: 58 [0/1900 (0%)]\tLoss: 0.002672\n",
            "Train Epoch: 58 [1600/1900 (5%)]\tLoss: 0.000605\n",
            "\n",
            "Test set: Average loss: 0.2092, Accuracy: 288/312 (92%)\n",
            "\n",
            "Train Epoch: 59 [0/1900 (0%)]\tLoss: 0.006887\n",
            "Train Epoch: 59 [1600/1900 (5%)]\tLoss: 0.000531\n",
            "\n",
            "Test set: Average loss: 0.1754, Accuracy: 292/312 (94%)\n",
            "\n",
            "Train Epoch: 60 [0/1900 (0%)]\tLoss: 0.004062\n",
            "Train Epoch: 60 [1600/1900 (5%)]\tLoss: 0.001992\n",
            "\n",
            "Test set: Average loss: 0.1467, Accuracy: 295/312 (95%)\n",
            "\n",
            "Train Time: 9140.352146863937\n",
            "\n",
            "Final Performance!\n",
            "Training Set:\n",
            "\n",
            "Test set: Average loss: 0.0032, Accuracy: 1898/1900 (100%)\n",
            "\n",
            "Validation Set:\n",
            "\n",
            "Test set: Average loss: 0.1474, Accuracy: 296/312 (95%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.14737917377780646, 0.9487179487179487)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trGKRHk90iPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "78a4f404-d03c-4132-cbb1-a5ab1cb87f0c"
      },
      "source": [
        "# Plot training with validation\n",
        "plt.plot(range(epochs), train_acc_lst, label='training')\n",
        "plt.plot(range(epochs), val_acc_lst, label='validation')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracies by Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "print(np.argmax(val_acc_lst))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQpi65hs0kqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22f1e0c-aa02-4ed2-c918-49b5c70927d1"
      },
      "source": [
        "# Loading best model parameters from file\n",
        "best_epoch = 50\n",
        "best_model_file_name = f'{model_save_path}/epoch{best_epoch}.pt'\n",
        "model.load_state_dict(torch.load(best_model_file_name))\n",
        "# run on test data\n",
        "_, test_acc = test(model, device, test_data)\n",
        "print('[Finished] Test-acc: %.3f' % (test_acc))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5172, Accuracy: 395/449 (88%)\n",
            "\n",
            "[Finished] Test-acc: 0.880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPKfked1xKu_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX-Dp9S5CP6J"
      },
      "source": [
        "## Next\n",
        "\n",
        "Now that you have learned to muster the power of transfer\n",
        "learning, to learn more about training a model on\n",
        "ImageNet, please read `this tutorial <dive_deep_imagenet.html>`__.\n",
        "\n",
        "The idea of transfer learning is the basis of\n",
        "`object detection <../examples_detection/index.html>`_ and\n",
        "`semantic segmentation <../examples_segmentation/index.html>`_,\n",
        "the next two chapters of our tutorial.\n",
        "\n",
        ".. |image-minc| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/datasets/MINC-2500.png\n",
        ".. |image-model| image:: https://zh.gluon.ai/_images/fine-tuning.svg\n",
        "\n"
      ]
    }
  ]
}