{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Aq3XceQyuik"
   },
   "source": [
    "# ConvGRU for Fluo Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C7UbnLHT9a-"
   },
   "source": [
    "This tutorial was very helpful, and some code here is taken from it: https://colab.research.google.com/drive/1B5KQvPySqYEa6XicRHdOwgv8fN1BrCgQ#scrollTo=LI6JtYwTt2HM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "Zt0YxNnZsa6O",
    "outputId": "10333251-5d1b-4161-998f-f584ee418138"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-8cb5678f-1ecd-4791-86a9-d98caef36167\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-8cb5678f-1ecd-4791-86a9-d98caef36167\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving utils.py to utils.py\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab/48919022\n",
    "import os\n",
    "from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('utils.py','wb').write(src)\n",
    "# print(os.path.abspath('utils.py'))\n",
    "import utils\n",
    "# help(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfb60oV6RWZD"
   },
   "source": [
    "Code to make sure the Drive mount later will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N8HKvxnIlevM"
   },
   "outputs": [],
   "source": [
    "# utils.prepare_drive()\n",
    "# https://github.com/googlecolab/colabtools/issues/1494\n",
    "!sed -i -e 's/enforce_single_parent:true/enforce_single_parent:true,metadata_cache_reset_counter:4/' /usr/local/lib/python3.6/dist-packages/google/colab/drive.py\n",
    "from google.colab import drive\n",
    "import importlib\n",
    "_ = importlib.reload(drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPvhmd4ucCYm"
   },
   "source": [
    "This code makes sure the shared drive mount (later on) will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_mpn7EYgcA4S"
   },
   "outputs": [],
   "source": [
    "# utils.prepare_drive()\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vChd6dfWyuil"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H29TGLErCP5r"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TexoEExhGRy"
   },
   "source": [
    "Install Conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v03ubARIfUz6",
    "outputId": "067117a7-81e8-41b4-f162-311012edf7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\n",
      "Python 3.6.9\n",
      "/env/python\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python --version\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zG53T9hI2nlC",
    "outputId": "b74f4e32-0d67-466f-dd80-c3824975da61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  1.7.0+cu101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Pytorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aO-06Rlqk8W9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PWecmH8yui0"
   },
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCMW-mPvwrcw",
    "outputId": "f1e27818-484c-4ca0-9bc3-19c4fd252c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive   \n",
    "\n",
    "drive.flush_and_unmount()\n",
    "# mount the google drive to my Colab session\n",
    "drive.mount('/content/gdrive')\n",
    "# use the google drive in my Colab session\n",
    "home_path = '/content/gdrive/Shared drives/Embryo_data'\n",
    "print(os.listdir(home_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVfTd_Uhyui4"
   },
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jarQcT4Myge3",
    "outputId": "57513c59-a8fa-47cb-abd7-19bb727ce7eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              t_num  first_anno_pol_time\n",
      "embryo_index                            \n",
      "1                21                   17\n",
      "3                21                   12\n",
      "12              143                   11\n",
      "13              143                   12\n",
      "16              143                   30\n",
      "18              143                   27\n",
      "19              143                   32\n",
      "24               21                   20\n",
      "39               21                   12\n",
      "40               21                    8\n",
      "42               21                   16\n",
      "45               21                   19\n",
      "46               21                   19\n",
      "47               21                   17\n",
      "49               21                   21\n",
      "50               21                   21\n",
      "52               21                   13\n",
      "[1, 3, 18, 50, 45, 49, 39, 47, 12, 40, 52, 16, 24, 42]\n",
      "[46]\n",
      "[13, 19]\n",
      "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/3d\n",
      "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/3d/train\n",
      "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/3d/val\n",
      "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/3d/test\n"
     ]
    }
   ],
   "source": [
    "# Fixing the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Available high quality data\n",
    "embryo_inds = [1, 3, 12, 13, 16, 18, 19, 24, 39, 40, 42, 45, 46, 47, 49, 50, 52]\n",
    "\n",
    "# Directory of the processed *.npy files\n",
    "processed_path = f'{home_path}/processed'\n",
    "polar_processed_path = f'{processed_path}/polarization'\n",
    "\n",
    "video_time_info, train_embryos, val_embryos, test_embryos = utils.split_train_test_val(home_path, embryo_inds)\n",
    "\n",
    "z_agg_mode = \"3 Z Slices Around Middle\"\n",
    "data_path = f'{home_path}/data/fluo_data'\n",
    "pol_path = f'{processed_path}/polarization'\n",
    "\n",
    "save_path = f'{processed_path}/fluo_data/3d'\n",
    "train_path = os.path.join(save_path, 'train')\n",
    "val_path = os.path.join(save_path, 'val')\n",
    "test_path = os.path.join(save_path, 'test')\n",
    "\n",
    "print(save_path)\n",
    "print(train_path)\n",
    "print(val_path)\n",
    "print(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAJrlvKDyui8"
   },
   "source": [
    "### Save NP Data as PNG for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "4DeDWzqZ1xKt",
    "outputId": "32d222f9-245b-4528-eb81-568afd1fbd0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 21\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-63727f11bdbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_time_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_nps_subset_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embryos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'per_embryo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'middle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_slices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_nps_subset_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embryos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'per_embryo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'middle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_slices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_nps_subset_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_embryos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'per_embryo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'middle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_slices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36msave_nps_subset_z\u001b[0;34m(embryos, save_path, specs, window, normalize, mode, n_slices, pol_subdir)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mslices_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{save_dir}/embryo_{embryo_idx}_z{mode}{n_slices}_t{t}_pol{pol_suffix}.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# memory optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/3d/train/0/embryo_1_zmiddle3_t0_pol.npy'"
     ]
    }
   ],
   "source": [
    "# Actually create the images\n",
    "specs = (data_path, pol_path, video_time_info)\n",
    "\n",
    "utils.save_nps_subset_z(train_embryos, train_path, specs, window=None, normalize='per_embryo', mode='middle', n_slices=3)\n",
    "utils.save_nps_subset_z(test_embryos, test_path, specs, window=None, normalize='per_embryo', mode='middle', n_slices=3)\n",
    "utils.save_nps_subset_z(val_embryos, val_path, specs, window=None, normalize='per_embryo', mode='middle', n_slices=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhHeN0l4CP5y"
   },
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVQCpNXYCP53"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "epochs = 15\n",
    "lr = 0.001\n",
    "per_device_batch_size = 20\n",
    "momentum = 0.9\n",
    "wd = 0.0001\n",
    "\n",
    "lr_factor = 0.75\n",
    "lr_steps = [10, 20, 30, np.inf]\n",
    "\n",
    "num_workers = 1\n",
    "num_gpus = 1\n",
    "gpus = [i for i in range(num_gpus)]\n",
    "\n",
    "device = torch.device(gpus[0]) if num_gpus > 0 else [torch.device('cpu')]\n",
    "\n",
    "batch_size = per_device_batch_size * max(num_gpus, 1)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THLMmIhfCP56"
   },
   "source": [
    "Things to keep in mind:\n",
    "\n",
    "1. ``epochs = 5`` is just for this tutorial with the tiny dataset. please change it to a larger number in your experiments, for instance 40.\n",
    "2. ``per_device_batch_size`` is also set to a small number. In your experiments you can try larger number like 64.\n",
    "3. remember to tune ``num_gpus`` and ``num_workers`` according to your machine.\n",
    "4. A pre-trained model is already in a pretty good status. So we can start with a small ``lr``.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "In transfer learning, data augmentation can also help.\n",
    "We use the following augmentation in training:\n",
    "\n",
    "2. Randomly crop the image and resize it to 224x224\n",
    "3. Randomly flip the image horizontally\n",
    "4. Randomly jitter color and add noise\n",
    "5. Transpose the data from height*width*num_channels to num_channels*height*width, and map values from [0, 255] to [0, 1]\n",
    "6. Normalize with the mean and standard deviation from the ImageNet dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYgiHxUECP56"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(600),\n",
    "    transforms.CenterCrop(512),\n",
    "\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomLighting(0.1), # Add AlexNet-style PCA-based noise to an image... need to implement: https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L183\n",
    "    # transforms.functional.adjust_contrast(contrast_factor=0.9 + np.random.random_sample()*0.2),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mRtApbiyujH"
   },
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OKZ2hq1CP59"
   },
   "source": [
    "With the data augmentation functions, we can define our data loaders:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "226lVHP2CP5-"
   },
   "outputs": [],
   "source": [
    "def check_npy(filename):\n",
    "    if filename.lower().endswith('.npy'):\n",
    "        return True\n",
    "    print(filename)\n",
    "    return False\n",
    "\n",
    "def npy_loader(path):\n",
    "    sample = torch.from_numpy(np.load(path))\n",
    "    return sample\n",
    "    \n",
    "train_data = torchvision.datasets.DatasetFolder(root=train_path, loader=npy_loader, is_valid_file=check_npy)\n",
    "val_data = torchvision.datasets.DatasetFolder(root=val_path, loader=npy_loader, is_valid_file=check_npy)\n",
    "test_data = torchvision.datasets.DatasetFolder(root=test_path, loader=npy_loader, is_valid_file=check_npy)\n",
    "\n",
    "# train_data = torchvision.datasets.ImageFolder(root=train_path, transform=transform, target_transform=None, is_valid_file=check_image)\n",
    "# val_data = torchvision.datasets.ImageFolder(root=val_path, transform=transform, target_transform=None, is_valid_file=check_image)\n",
    "# test_data = torchvision.datasets.ImageFolder(root=test_path, transform=transform, target_transform=None, is_valid_file=check_image)\n",
    "\n",
    "print(\"Train data: \", train_data)\n",
    "print(\"Validation data: \", val_data)\n",
    "print(\"Test data: \", test_data)\n",
    "\n",
    "# Define data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, sampler=None, pin_memory=False, drop_last=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, sampler=None, num_workers=num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, sampler=None, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, sampler=None, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "classes = ('unpolarized', 'polarized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5ihNL_357yq"
   },
   "source": [
    "Visualize some training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCCEp0yzySSD"
   },
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "# next(dataiter)\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    \"\"\"\n",
    "    :param img: (PyTorch Tensor)\n",
    "    \"\"\"\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5     \n",
    "    # Convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize = (20,8))\n",
    "    # Color channel first -> color channel last\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "  \n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('{:>10}'.format(classes[labels[j]]) for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63nMwayBKRWZ"
   },
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90qsb8ruaBER"
   },
   "outputs": [],
   "source": [
    "# images.float().dtype\n",
    "transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IkfDxPxCP6B"
   },
   "source": [
    "\n",
    "Note that only ``train_data`` uses ``transform_train``, while\n",
    "``val_data`` and ``test_data`` use ``transform_test`` to produce deterministic\n",
    "results for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaQn1kxIyujM"
   },
   "source": [
    "### Initialize Optimizer and Some Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pw5oJ6PX3r8w"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    # it combines softmax with negative log likelihood loss\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gVssLY630jD"
   },
   "outputs": [],
   "source": [
    "# duplicate from before, but maybe it's useful to be able to change the batch size dynamically during training...? mini-batches?\n",
    "\n",
    "def get_train_loader(batch_size):\n",
    "    return torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, sampler=None,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "def get_val_loader(batch_size):\n",
    "# Use larger batch size for validation to speed up computation\n",
    "    return torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, sampler=None,\n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJG2Rg_NCP6G"
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "Following is the main training loop. It is the same as the loop in\n",
    "`CIFAR10 <dive_deep_cifar10.html>`__\n",
    "and ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7mf0H-N4CmG"
   },
   "outputs": [],
   "source": [
    "def train(net, batch_size, n_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a neural network and print statistics of the training\n",
    "    \n",
    "    :param net: (PyTorch Neural Network)\n",
    "    :param batch_size: (int)\n",
    "    :param n_epochs: (int)  Number of iterations on the training set\n",
    "    :param learning_rate: (float) learning rate used by the optimizer\n",
    "    \"\"\"\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"n_epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_minibatches = len(train_loader)\n",
    "    print(\"number of minibaches: \", n_minibatches)\n",
    "\n",
    "    criterion, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    # Init variables used for plotting the loss\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    # train_history_acc = []\n",
    "    # val_history_acc = []\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    best_error = np.inf\n",
    "    best_model_path = \"best_model.pth\"\n",
    "\n",
    "    # mutli-gpu\n",
    "    if gpus is not None and len(gpus) > 1:\n",
    "        print('multi-gpu ', gpus)\n",
    "        net = torch.nn.DataParallel(net, device_ids=gpus)  # model becomes `torch.nn.DataParallel` w/ model.module being the orignal `torch.nn.Module`\n",
    "        net = net.to(device)\n",
    "\n",
    "    # Move model to gpu if possible\n",
    "    else:\n",
    "        net = net.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        print_every = n_minibatches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Move tensors to correct device\n",
    "            inputs, labels = inputs.to(device=device, dtype=torch.float), labels.to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # print every 10th of epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:    \n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
    "                      time.time() - start_time))\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "\n",
    "        train_history.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        total_val_loss = 0\n",
    "        # Do a pass on the validation set\n",
    "        # We don't need to compute gradient,\n",
    "        # we save memory and computation using torch.no_grad()\n",
    "        with torch.no_grad():\n",
    "            val_loader = get_val_loader(batch_size)\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move tensors to correct device\n",
    "                inputs, labels = inputs.to(device=device, dtype=torch.float), labels.to(device=device)\n",
    "                # Forward pass\n",
    "                predictions = net(inputs)\n",
    "                val_loss = criterion(predictions, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "            \n",
    "        val_history.append(total_val_loss / len(val_loader))\n",
    "        # Save model that performs best on validation set\n",
    "        if total_val_loss < best_error:\n",
    "            best_error = total_val_loss\n",
    "            torch.save(net.state_dict(), best_model_path)\n",
    "\n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "\n",
    "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "    \n",
    "    # Load best model\n",
    "    net.load_state_dict(torch.load(best_model_path))\n",
    "    \n",
    "    return train_history, val_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S8XS0ijo4ROm"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConvGRU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa43d58f0349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m net = ConvGRU(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0;31m#input_size=(height, width),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConvGRU' is not defined"
     ]
    }
   ],
   "source": [
    "net = convgru.ConvGRU(\n",
    "                #input_size=(height, width),\n",
    "                input_dim=3,\n",
    "                hidden_dim=[20, 50],\n",
    "                kernel_size=(3, 3),\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                bias=True,\n",
    "                return_all_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history, val_history = train(net, batch_size=batch_size, n_epochs=10, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdvNUTnVT1LF"
   },
   "source": [
    "Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5ZhFZOETNHJ"
   },
   "outputs": [],
   "source": [
    "def dataset_accuracy(net, data_loader, name=\"\", print_first_k_mistakes=0):\n",
    "    net = net.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    to_print = []\n",
    "    for images_loader, labels_loader in data_loader:\n",
    "        images, labels = images_loader.to(device=device, dtype=torch.float), labels_loader.to(device=device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if print_first_k_mistakes > 0:\n",
    "            for idx, match in enumerate(zip(predicted, labels)):\n",
    "                if match[0] != match[1]:\n",
    "                    error = \"Actually unpolarized:\"\n",
    "                    if match[1] == 1:\n",
    "                        error = \"Actually polarized:\"\n",
    "                    to_print.append((error, images_loader[idx]))\n",
    "                    print_first_k_mistakes -= 1\n",
    "    accuracy = 100 * float(correct) / total\n",
    "    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n",
    "\n",
    "    if len(to_print) > 0:\n",
    "        print('\\n'.join('{:>10}'.format(label[0]) for label in to_print))\n",
    "        imshow(torchvision.utils.make_grid([img[1] for img in to_print]))\n",
    "\n",
    "def train_set_accuracy(net):\n",
    "    dataset_accuracy(net, train_loader, \"train\")\n",
    "\n",
    "def val_set_accuracy(net):\n",
    "    dataset_accuracy(net, val_loader, \"validation\")  \n",
    "    \n",
    "def test_set_accuracy(net):\n",
    "    dataset_accuracy(net, test_loader, \"test\", 10)\n",
    "\n",
    "def compute_accuracy(net):\n",
    "    train_set_accuracy(net)\n",
    "    val_set_accuracy(net)\n",
    "    test_set_accuracy(net)\n",
    "    \n",
    "print(\"Computing accuracy...\")\n",
    "compute_accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4tY-o1fyujS"
   },
   "source": [
    "### Plot results (these need to be refactored)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rw4EdxjbI9zt"
   },
   "outputs": [],
   "source": [
    "def plot_losses(train_history, val_history):\n",
    "    x = np.arange(1, len(train_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.plot(x, train_history, '-r', label=\"Training loss\", linewidth=2)\n",
    "    plt.plot(x, val_history, '-b', label=\"Validation loss\", linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"Training and Validation Loss History\" + \"(\" + z_agg_mode + \")\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    :param cm: (numpy matrix) confusion matrix\n",
    "    :param classes: [str]\n",
    "    :param normalize: (bool)\n",
    "    :param title: (str)\n",
    "    :param cmap: (matplotlib color map)\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(8, 8), dpi=100)   \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCQdZHEayujT"
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(np.arange(0,epochs,1),train_acc_lst,'r')\n",
    "# plt.ylabel('Training Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.title('CNN Classify Fluo Polarization')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebW8cv5kyujU"
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(np.arange(0,epochs,1),train_loss_lst,'r')\n",
    "# plt.ylabel('Training Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.title('CNN Classify Fluo Polarization')\n",
    "# plt.show()\n",
    "\n",
    "plot_losses(train_history, val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xWHwM3AyujV"
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(np.arange(0,epochs,1),val_acc_lst,'r')\n",
    "# plt.ylabel('Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.title('CNN Classify Fluo Polarization')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oxvgfhcHGX1"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "test_batch_size = batch_size\n",
    "\n",
    "def accuracy_per_class(net):\n",
    "    net = net.to(device)\n",
    "    n_classes = 2\n",
    "    # (real, predicted)\n",
    "    confusion_matrix = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device=device, dtype=torch.float), labels.to(device=device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for i in range(test_batch_size):\n",
    "            confusion_matrix[labels[i], predicted[i]] += 1\n",
    "            label = labels[i]\n",
    "\n",
    "    print(\"{:<10} {:^10}\".format(\"Class\", \"Accuracy (%)\"))\n",
    "    for i in range(n_classes):\n",
    "        class_total = confusion_matrix[i, :].sum()\n",
    "        class_correct = confusion_matrix[i, i]\n",
    "        percentage_correct = 100.0 * float(class_correct) / class_total\n",
    "        \n",
    "        print('{:<10} {:^10.2f}'.format(classes[i], percentage_correct))\n",
    "    return confusion_matrix\n",
    "\n",
    "confusion_matrix = accuracy_per_class(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-E3l96gYXal"
   },
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(confusion_matrix, classes, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(confusion_matrix, classes,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzvwZigS5YdK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classify_fluo_CNN_3D_torch_drive_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
