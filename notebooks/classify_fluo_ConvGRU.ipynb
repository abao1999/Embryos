{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "classify_fluo_ConvGRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aq3XceQyuik"
      },
      "source": [
        "# ConvGRU for Fluo Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C7UbnLHT9a-"
      },
      "source": [
        "This tutorial was very helpful, and some code here is taken from it: https://colab.research.google.com/drive/1B5KQvPySqYEa6XicRHdOwgv8fN1BrCgQ#scrollTo=LI6JtYwTt2HM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "Zt0YxNnZsa6O",
        "outputId": "25906980-2380-43cf-b362-00f15bc2c34e"
      },
      "source": [
        "# https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab/48919022\n",
        "import os\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('utils.py','wb').write(src)\n",
        "# print(os.path.abspath('utils.py'))\n",
        "import utils\n",
        "# help(utils)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-60aacbf6-8165-4642-ac66-6b3737f40733\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-60aacbf6-8165-4642-ac66-6b3737f40733\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils (4).py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfb60oV6RWZD"
      },
      "source": [
        "Code to make sure the Drive mount later will work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8HKvxnIlevM"
      },
      "source": [
        "# utils.prepare_drive()\n",
        "# https://github.com/googlecolab/colabtools/issues/1494\n",
        "!sed -i -e 's/enforce_single_parent:true/enforce_single_parent:true,metadata_cache_reset_counter:4/' /usr/local/lib/python3.6/dist-packages/google/colab/drive.py\n",
        "from google.colab import drive\n",
        "import importlib\n",
        "_ = importlib.reload(drive)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPvhmd4ucCYm"
      },
      "source": [
        "This code makes sure the shared drive mount (later on) will work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mpn7EYgcA4S"
      },
      "source": [
        "# utils.prepare_drive()\n",
        "# from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vChd6dfWyuil"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H29TGLErCP5r"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TexoEExhGRy"
      },
      "source": [
        "Install Conda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v03ubARIfUz6",
        "outputId": "fe181d3b-e910-471e-eb6d-cc9fa2c3e1bb"
      },
      "source": [
        "!which python\n",
        "!python --version\n",
        "!echo $PYTHONPATH"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python\n",
            "Python 3.6.9\n",
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG53T9hI2nlC",
        "outputId": "0fee4894-3746-4498-bc38-16f0822a63f6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"Pytorch version: \", torch.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version:  1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO-06Rlqk8W9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, time, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PWecmH8yui0"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCMW-mPvwrcw",
        "outputId": "5204044b-e582-4db3-f5af-b401a573ff9a"
      },
      "source": [
        "import os\n",
        "from google.colab import drive   \n",
        "\n",
        "drive.flush_and_unmount()\n",
        "# mount the google drive to my Colab session\n",
        "drive.mount('/content/gdrive')\n",
        "# use the google drive in my Colab session\n",
        "home_path = '/content/gdrive/Shared drives/Embryo_data'\n",
        "print(os.listdir(home_path))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "['mxnet_cnn2d_embryo_58_fine_tune_data_aug_ResNet50_v2_order_random.ipynb', 'Embryo3', 'Embryo12', 'Embryo13', 'Embryo16', 'Embryo19', 'Embryo18', 'Embryo24', 'Embryo39', 'Embryo42', 'Embryo46', 'Embryo47', 'Embryo23', 'Embryo33', 'Embryo25', 'Embryo95', 'Embryo97', 'Embryo96', 'Embryo98', 'Embryo101', 'Embryo99', 'Embryo100', 'Embryo102', 'Embryo76', 'Embryo78', 'Embryo81', 'Embryo79', 'Embryo80', 'Embryo84', 'Embryo85', 'Embryo87', 'Embryo88', 'Embryo92', 'Embryo94', 'Embryo93', 'embryo_info_CS101.xlsx', 'data', 'processed', 'models', 'pix2pix_PyTorch-GAN.ipynb', 'annotation.xlsx', 'Embryo110', 'Embryo109', 'Embryo111', 'Embryo113', 'Embryo112', 'Embryo114', 'Embryo116', 'Embryo115', 'Embryo117', 'Embryo118', 'Embryo119', 'Embryo120', 'Embryo103', 'Embryo104', 'Embryo105', 'Embryo107', 'Embryo106', 'Embryo108']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVfTd_Uhyui4"
      },
      "source": [
        "### Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jarQcT4Myge3",
        "outputId": "209a0e7f-efc7-4c64-8887-2d76a2b20132"
      },
      "source": [
        "# Fixing the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Available high quality data\n",
        "embryo_inds = [1, 3, 12, 13, 16, 18, 19, 24, 39, 40, 42, 45, 46, 47, 49, 50, 52]\n",
        "\n",
        "# Directory of the processed *.npy files\n",
        "processed_path = f'{home_path}/processed'\n",
        "polar_processed_path = f'{processed_path}/polarization'\n",
        "\n",
        "video_time_info, train_embryos, val_embryos, test_embryos = utils.split_train_test_val(home_path, embryo_inds)\n",
        "\n",
        "# z_agg_mode = \"3 Z Slices Around Middle\"\n",
        "z_agg_mode = \"Max Z Timeseries\"\n",
        "data_path = f'{home_path}/data/fluo_data'\n",
        "pol_path = f'{processed_path}/polarization'\n",
        "\n",
        "# save_path = f'{processed_path}/fluo_data/3d'\n",
        "save_path = f'{processed_path}/fluo_data/max'\n",
        "train_path = os.path.join(save_path, 'train')\n",
        "val_path = os.path.join(save_path, 'val')\n",
        "test_path = os.path.join(save_path, 'test')\n",
        "\n",
        "print(save_path)\n",
        "print(train_path)\n",
        "print(val_path)\n",
        "print(test_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              t_num  first_anno_pol_time\n",
            "embryo_index                            \n",
            "1                21                   17\n",
            "3                21                   12\n",
            "12              143                   11\n",
            "13              143                   12\n",
            "16              143                   30\n",
            "18              143                   27\n",
            "19              143                   32\n",
            "24               21                   20\n",
            "39               21                   12\n",
            "40               21                    8\n",
            "42               21                   16\n",
            "45               21                   19\n",
            "46               21                   19\n",
            "47               21                   17\n",
            "49               21                   21\n",
            "50               21                   21\n",
            "52               21                   13\n",
            "[1, 3, 18, 50, 45, 49, 39, 47, 12, 40, 52, 16, 24, 42]\n",
            "[46]\n",
            "[13, 19]\n",
            "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/max\n",
            "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/max/train\n",
            "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/max/val\n",
            "/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/max/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAJrlvKDyui8"
      },
      "source": [
        "### Save NP Data in Correct Format for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjL3boc_qu5h"
      },
      "source": [
        "#### Sanity Check on Embryo npy Shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HTzb-_AmZEP",
        "outputId": "c1ecc06d-541b-4b5f-f12e-459ea4373fdd"
      },
      "source": [
        "embryo_idx = 3\n",
        "embryo_path = f'{data_path}/embryo{embryo_idx}.npy'\n",
        "embryo_pol_path = f'{pol_path}/embryo{embryo_idx}.npy'\n",
        "\n",
        "embryo = np.load(embryo_path)\n",
        "embryo_pol = np.squeeze(np.load(embryo_pol_path)).astype(int)\n",
        "\n",
        "# pre-normalization steps\n",
        "# embryo is currently np.float32, but for 3D input\n",
        "# this conversion step exceeds Colab RAM usage\n",
        "embryo = embryo.astype(np.float64)\n",
        "\n",
        "# print(embryo.shape)\n",
        "\n",
        "# normalize the data to 0 - 1\n",
        "normalize = 'per_embryo'\n",
        "if normalize == 'per_embryo': # max val over the full embryo\n",
        "    embryo /= np.max(embryo)\n",
        "elif normalize == 'per_timestep': # max val over each timestep\n",
        "    embryo /= np.max(embryo, axis=(0,1))\n",
        "elif type(normalize) is not str: # a fixed numerical factor\n",
        "    embryo /= normalize\n",
        "embryo = 255 * embryo # Now scale by 255\n",
        "embryo = embryo.astype(np.uint8)\n",
        "\n",
        "print(\"(z, h, w, t)\")\n",
        "print(embryo.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(z, h, w, t)\n",
            "(21, 512, 512, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DeDWzqZ1xKt"
      },
      "source": [
        "# Actually create the train files\n",
        "specs = (data_path, pol_path, video_time_info)\n",
        "\n",
        "# TODO: write function to select z index and use whole timeseries\n",
        "# for example, if max, then make it into shape (t, c, h, w)\n",
        "# corresponding to t range (start with all t, could split into before and after polarization though)\n",
        "# ..., c for channels (z slices), and height h and width w \n",
        "\n",
        "# utils.save_nps_subset_z(train_embryos, train_path, specs, window=None, normalize='per_embryo', mode='middle', n_slices=3)\n",
        "# utils.save_nps_subset_z(test_embryos, test_path, specs, window=None, normalize='per_embryo', mode='middle', n_slices=3)\n",
        "# utils.save_nps_subset_z(val_embryos, val_path, specs, window=None, normalize='per_embryo', mode='middle', n_slices=3)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhHeN0l4CP5y"
      },
      "source": [
        "### Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQCpNXYCP53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb66f39e-8c48-4462-e0ab-61072056fa5b"
      },
      "source": [
        "num_classes = 2\n",
        "\n",
        "epochs = 15\n",
        "lr = 0.001\n",
        "per_device_batch_size = 20\n",
        "momentum = 0.9\n",
        "wd = 0.0001\n",
        "\n",
        "lr_factor = 0.75\n",
        "lr_steps = [10, 20, 30, np.inf]\n",
        "\n",
        "num_workers = 1\n",
        "num_gpus = 1\n",
        "gpus = [i for i in range(num_gpus)]\n",
        "\n",
        "device = torch.device(gpus[0]) if num_gpus > 0 else [torch.device('cpu')]\n",
        "\n",
        "batch_size = per_device_batch_size * max(num_gpus, 1)\n",
        "print(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THLMmIhfCP56"
      },
      "source": [
        "Things to keep in mind:\n",
        "\n",
        "1. ``epochs = 5`` is just for this tutorial with the tiny dataset. please change it to a larger number in your experiments, for instance 40.\n",
        "2. ``per_device_batch_size`` is also set to a small number. In your experiments you can try larger number like 64.\n",
        "3. remember to tune ``num_gpus`` and ``num_workers`` according to your machine.\n",
        "4. A pre-trained model is already in a pretty good status. So we can start with a small ``lr``.\n",
        "\n",
        "### Data Augmentation\n",
        "\n",
        "In transfer learning, data augmentation can also help.\n",
        "We use the following augmentation in training:\n",
        "\n",
        "2. Randomly crop the image and resize it to 224x224\n",
        "3. Randomly flip the image horizontally\n",
        "4. Randomly jitter color and add noise\n",
        "5. Transpose the data from height*width*num_channels to num_channels*height*width, and map values from [0, 255] to [0, 1]\n",
        "6. Normalize with the mean and standard deviation from the ImageNet dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgiHxUECP56"
      },
      "source": [
        "# TODO: tranform the \"video\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(600),\n",
        "    transforms.CenterCrop(512),\n",
        "\n",
        "    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    # transforms.RandomLighting(0.1), # Add AlexNet-style PCA-based noise to an image... need to implement: https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L183\n",
        "    # transforms.functional.adjust_contrast(contrast_factor=0.9 + np.random.random_sample()*0.2),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mRtApbiyujH"
      },
      "source": [
        "### Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OKZ2hq1CP59"
      },
      "source": [
        "With the data augmentation functions, we can define our data loaders:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226lVHP2CP5-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "62a1fe2b-0d9c-4b14-a3bc-69937d9caeba"
      },
      "source": [
        "def check_npy_in_train(filename):\n",
        "    if not filename.lower().endswith('.npy'):\n",
        "        return False\n",
        "    if int(\"\".join(list(filter(lambda x: x.isdigit(), filename)))) in train_embryos:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def check_npy_in_val(filename):\n",
        "    if not filename.lower().endswith('.npy'):\n",
        "        return False\n",
        "    if int(\"\".join(list(filter(lambda x: x.isdigit(), filename)))) in val_embryos:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def check_npy_in_test(filename):\n",
        "    if not filename.lower().endswith('.npy'):\n",
        "        return False\n",
        "    if int(\"\".join(list(filter(lambda x: x.isdigit(), filename)))) in test_embryos:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def check_npy(filename):\n",
        "    if filename.lower().endswith('.npy'):\n",
        "        return True\n",
        "    print(filename)\n",
        "    return False\n",
        "\n",
        "def check_image(path):\n",
        "    try:\n",
        "        im = Image.open(path)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def check_png(path):\n",
        "    return filename.lower().endswith('.png')\n",
        "\n",
        "def npy_loader(path):\n",
        "    sample = torch.from_numpy(np.load(path))\n",
        "    return sample\n",
        "    \n",
        "# TODO: Segment Embryo_data/data/fluo_data folders into subfolders for each class... how to define though?\n",
        "# .... before and after polarization?\n",
        "train_data = torchvision.datasets.DatasetFolder(root=data_path, loader=npy_loader, is_valid_file=check_npy_in_train)\n",
        "val_data = torchvision.datasets.DatasetFolder(root=data_path, loader=npy_loader, is_valid_file=check_npy_in_val)\n",
        "test_data = torchvision.datasets.DatasetFolder(root=data_path, loader=npy_loader, is_valid_file=check_npy_in_test)\n",
        "\n",
        "# train_data = torchvision.datasets.ImageFolder(root=train_path, transform=transform, target_transform=None, is_valid_file=None)\n",
        "# val_data = torchvision.datasets.ImageFolder(root=val_path, transform=transform, target_transform=None, is_valid_file=None)\n",
        "# test_data = torchvision.datasets.ImageFolder(root=test_path, transform=transform, target_transform=None, is_valid_file=None)\n",
        "\n",
        "print(\"Train data: \", train_data)\n",
        "print(\"Validation data: \", val_data)\n",
        "print(\"Test data: \", test_data)\n",
        "\n",
        "# Define data loaders\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, sampler=None, pin_memory=False, drop_last=False)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, sampler=None, num_workers=num_workers)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, sampler=None, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, sampler=None, num_workers=num_workers, drop_last=True)\n",
        "\n",
        "classes = ('unpolarized', 'polarized')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7ac538734541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpy_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_npy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpy_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_npy_in_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpy_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_npy_in_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Supported extensions are: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: /content/gdrive/Shared drives/Embryo_data/data/fluo_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ihNL_357yq"
      },
      "source": [
        "Visualize some training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCCEp0yzySSD"
      },
      "source": [
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "# next(dataiter)\n",
        "\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    \"\"\"\n",
        "    :param img: (PyTorch Tensor)\n",
        "    \"\"\"\n",
        "    # unnormalize\n",
        "    img = img / 2 + 0.5     \n",
        "    # Convert tensor to numpy array\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize = (20,8))\n",
        "    # Color channel first -> color channel last\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "  \n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('{:>10}'.format(classes[labels[j]]) for j in range(batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63nMwayBKRWZ"
      },
      "source": [
        "images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90qsb8ruaBER"
      },
      "source": [
        "images.float().dtype\n",
        "images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IkfDxPxCP6B"
      },
      "source": [
        "\n",
        "Note that only ``train_data`` uses ``transform_train``, while\n",
        "``val_data`` and ``test_data`` use ``transform_test`` to produce deterministic\n",
        "results for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErPfwVZkdFMd"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaQn1kxIyujM"
      },
      "source": [
        "### Initialize Optimizer and Some Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw5oJ6PX3r8w"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    # it combines softmax with negative log likelihood loss\n",
        "    criterion = nn.CrossEntropyLoss()  \n",
        "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    return criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gVssLY630jD"
      },
      "source": [
        "# duplicate from before, but maybe it's useful to be able to change the batch size dynamically during training...? mini-batches?\n",
        "\n",
        "def get_train_loader(batch_size):\n",
        "    return torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, sampler=None,\n",
        "                                              num_workers=num_workers)\n",
        "\n",
        "def get_val_loader(batch_size):\n",
        "# Use larger batch size for validation to speed up computation\n",
        "    return torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, sampler=None,\n",
        "                                          num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJG2Rg_NCP6G"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Following is the main training loop. It is the same as the loop in\n",
        "`CIFAR10 <dive_deep_cifar10.html>`__\n",
        "and ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7mf0H-N4CmG"
      },
      "source": [
        "def train(net, batch_size, n_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a neural network and print statistics of the training\n",
        "    \n",
        "    :param net: (PyTorch Neural Network)\n",
        "    :param batch_size: (int)\n",
        "    :param n_epochs: (int)  Number of iterations on the training set\n",
        "    :param learning_rate: (float) learning rate used by the optimizer\n",
        "    \"\"\"\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"n_epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_minibatches = len(train_loader)\n",
        "    print(\"number of minibaches: \", n_minibatches)\n",
        "\n",
        "    criterion, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    # Init variables used for plotting the loss\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    # train_history_acc = []\n",
        "    # val_history_acc = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    best_error = np.inf\n",
        "    best_model_path = \"best_model.pth\"\n",
        "\n",
        "    # mutli-gpu\n",
        "    if gpus is not None and len(gpus) > 1:\n",
        "        print('multi-gpu ', gpus)\n",
        "        net = torch.nn.DataParallel(net, device_ids=gpus)  # model becomes `torch.nn.DataParallel` w/ model.module being the orignal `torch.nn.Module`\n",
        "        net = net.to(device)\n",
        "\n",
        "    # Move model to gpu if possible\n",
        "    else:\n",
        "        net = net.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        print_every = n_minibatches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            # Move tensors to correct device\n",
        "            inputs, labels = inputs.to(device=device, dtype=torch.float), labels.to(device=device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # print every 10th of epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:    \n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
        "                      time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "\n",
        "        train_history.append(total_train_loss / len(train_loader))\n",
        "\n",
        "        total_val_loss = 0\n",
        "        # Do a pass on the validation set\n",
        "        # We don't need to compute gradient,\n",
        "        # we save memory and computation using torch.no_grad()\n",
        "        with torch.no_grad():\n",
        "            val_loader = get_val_loader(batch_size)\n",
        "            for inputs, labels in val_loader:\n",
        "                # Move tensors to correct device\n",
        "                inputs, labels = inputs.to(device=device, dtype=torch.float), labels.to(device=device)\n",
        "                # Forward pass\n",
        "                predictions = net(inputs)\n",
        "                val_loss = criterion(predictions, labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "            \n",
        "        val_history.append(total_val_loss / len(val_loader))\n",
        "        # Save model that performs best on validation set\n",
        "        if total_val_loss < best_error:\n",
        "            best_error = total_val_loss\n",
        "            torch.save(net.state_dict(), best_model_path)\n",
        "\n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "\n",
        "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    \n",
        "    # Load best model\n",
        "    net.load_state_dict(torch.load(best_model_path))\n",
        "    \n",
        "    return train_history, val_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LAgojaZdFMd"
      },
      "source": [
        "### Initialize Model and Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJgj7jwxhkY3"
      },
      "source": [
        "# https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab/48919022\n",
        "src = list(files.upload().values())[0]\n",
        "open('convgru.py','wb').write(src)\n",
        "# print(os.path.abspath('utils.py'))\n",
        "import utils\n",
        "# help(utils)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8XS0ijo4ROm"
      },
      "source": [
        "from convgru import *\n",
        "\n",
        "net = ConvGRU(\n",
        "                #input_size=(height, width),\n",
        "                in_channels=3,\n",
        "                hidden_channels=[20, 50],\n",
        "                kernel_size=(3, 3),\n",
        "                num_layers=2,\n",
        "                batch_first=True,\n",
        "                bias=True,\n",
        "                return_all_layers=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lMNkBbLdFMe"
      },
      "source": [
        "train_history, val_history = train(net, batch_size=batch_size, n_epochs=10, learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdvNUTnVT1LF"
      },
      "source": [
        "Evaluate the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5ZhFZOETNHJ"
      },
      "source": [
        "def dataset_accuracy(net, data_loader, name=\"\", print_first_k_mistakes=0):\n",
        "    net = net.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    to_print = []\n",
        "    for images_loader, labels_loader in data_loader:\n",
        "        images, labels = images_loader.to(device=device, dtype=torch.float), labels_loader.to(device=device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "        if print_first_k_mistakes > 0:\n",
        "            for idx, match in enumerate(zip(predicted, labels)):\n",
        "                if match[0] != match[1]:\n",
        "                    error = \"Actually unpolarized:\"\n",
        "                    if match[1] == 1:\n",
        "                        error = \"Actually polarized:\"\n",
        "                    to_print.append((error, images_loader[idx]))\n",
        "                    print_first_k_mistakes -= 1\n",
        "    accuracy = 100 * float(correct) / total\n",
        "    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n",
        "\n",
        "    if len(to_print) > 0:\n",
        "        print('\\n'.join('{:>10}'.format(label[0]) for label in to_print))\n",
        "        imshow(torchvision.utils.make_grid([img[1] for img in to_print]))\n",
        "\n",
        "def train_set_accuracy(net):\n",
        "    dataset_accuracy(net, train_loader, \"train\")\n",
        "\n",
        "def val_set_accuracy(net):\n",
        "    dataset_accuracy(net, val_loader, \"validation\")  \n",
        "    \n",
        "def test_set_accuracy(net):\n",
        "    dataset_accuracy(net, test_loader, \"test\", 10)\n",
        "\n",
        "def compute_accuracy(net):\n",
        "    train_set_accuracy(net)\n",
        "    val_set_accuracy(net)\n",
        "    test_set_accuracy(net)\n",
        "    \n",
        "print(\"Computing accuracy...\")\n",
        "compute_accuracy(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4tY-o1fyujS"
      },
      "source": [
        "### Plot results (these need to be refactored)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw4EdxjbI9zt"
      },
      "source": [
        "def plot_losses(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=100)\n",
        "    plt.plot(x, train_history, '-r', label=\"Training loss\", linewidth=2)\n",
        "    plt.plot(x, val_history, '-b', label=\"Validation loss\", linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(\"Training and Validation Loss History\" + \"(\" + z_agg_mode + \")\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "    :param cm: (numpy matrix) confusion matrix\n",
        "    :param classes: [str]\n",
        "    :param normalize: (bool)\n",
        "    :param title: (str)\n",
        "    :param cmap: (matplotlib color map)\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        \n",
        "    plt.figure(figsize=(8, 8), dpi=100)   \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCQdZHEayujT"
      },
      "source": [
        "# plt.figure()\n",
        "# plt.plot(np.arange(0,epochs,1),train_acc_lst,'r')\n",
        "# plt.ylabel('Training Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.title('CNN Classify Fluo Polarization')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebW8cv5kyujU"
      },
      "source": [
        "# plt.figure()\n",
        "# plt.plot(np.arange(0,epochs,1),train_loss_lst,'r')\n",
        "# plt.ylabel('Training Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.title('CNN Classify Fluo Polarization')\n",
        "# plt.show()\n",
        "\n",
        "plot_losses(train_history, val_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xWHwM3AyujV"
      },
      "source": [
        "# plt.figure()\n",
        "# plt.plot(np.arange(0,epochs,1),val_acc_lst,'r')\n",
        "# plt.ylabel('Validation Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.title('CNN Classify Fluo Polarization')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oxvgfhcHGX1"
      },
      "source": [
        "import itertools\n",
        "\n",
        "test_batch_size = batch_size\n",
        "\n",
        "def accuracy_per_class(net):\n",
        "    net = net.to(device)\n",
        "    n_classes = 2\n",
        "    # (real, predicted)\n",
        "    confusion_matrix = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device=device, dtype=torch.float), labels.to(device=device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        for i in range(test_batch_size):\n",
        "            confusion_matrix[labels[i], predicted[i]] += 1\n",
        "            label = labels[i]\n",
        "\n",
        "    print(\"{:<10} {:^10}\".format(\"Class\", \"Accuracy (%)\"))\n",
        "    for i in range(n_classes):\n",
        "        class_total = confusion_matrix[i, :].sum()\n",
        "        class_correct = confusion_matrix[i, i]\n",
        "        percentage_correct = 100.0 * float(class_correct) / class_total\n",
        "        \n",
        "        print('{:<10} {:^10.2f}'.format(classes[i], percentage_correct))\n",
        "    return confusion_matrix\n",
        "\n",
        "confusion_matrix = accuracy_per_class(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-E3l96gYXal"
      },
      "source": [
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(confusion_matrix, classes, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(confusion_matrix, classes,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzvwZigS5YdK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}