{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "classify_fluo_CNN_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aq3XceQyuik"
      },
      "source": [
        "# CNN for Fluo Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vChd6dfWyuil"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H29TGLErCP5r"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVkEhqIhCeLH"
      },
      "source": [
        "# !pip install mxnet-cu101\n",
        "# !pip install gluoncv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TexoEExhGRy"
      },
      "source": [
        "Install Conda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v03ubARIfUz6",
        "outputId": "cf917c1a-369a-4299-e2e5-2c1e60c48a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!which python\n",
        "!python --version\n",
        "!echo $PYTHONPATH"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python\n",
            "Python 3.6.12 :: Anaconda, Inc.\n",
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTrQ-NBGgT-l"
      },
      "source": [
        "# # unset PYTHONPATH because it may interfere with Miniconda\n",
        "# %env PYTHONPATH="
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfmThZmRgSBT"
      },
      "source": [
        "# %%bash\n",
        "# MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "# MINICONDA_PREFIX=/usr/local\n",
        "# wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "# chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "# ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4vGCiaSgqK5"
      },
      "source": [
        "# !which conda # should return /usr/local/bin/conda\n",
        "# !conda --version # should return 4.5.4\n",
        "\n",
        "# # verify the Python executable isn't impacted\n",
        "# !which python # still returns /usr/local/bin/python\n",
        "\n",
        "# # Anaconda has slightly different version of Python\n",
        "# !python --version # now returns Python 3.6.5 :: Anaconda, Inc."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZpT42IvhRpt"
      },
      "source": [
        "# # update conda without updating Python to 3.7 or 3.8\n",
        "\n",
        "# %%bash\n",
        "# conda install --channel defaults conda python=3.6 --yes\n",
        "# conda update --channel defaults --all --yes"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtN-JfnHhgiY"
      },
      "source": [
        "# !conda --version # now returns 4.9.1\n",
        "# !python --version # now returns Python 3.6.12 :: Anaconda, Inc."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLCxtb1VhuEc"
      },
      "source": [
        "# # Append to sys.path... add directory where Conda will isntall packages to list of directories Python searches through\n",
        "# import sys\n",
        "# # print(sys.path)\n",
        "# # !ls /usr/local/lib/python3.6/dist-packages\n",
        "\n",
        "# # All packages installed with Conda will go into this directory, which we add to path below:\n",
        "# conda_pack_dir = \"/usr/local/lib/python3.6/site-packages\"\n",
        "# if conda_pack_dir not in sys.path:\n",
        "#     _ = (sys.path.append(cond_pack_dir))\n",
        "# print(sys.path)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsF2mJFWj8l1"
      },
      "source": [
        "Install Pytorch (Not actually necessary because colab comes wth Torch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWxMNamgjCyH"
      },
      "source": [
        "# !conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch --yes"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG53T9hI2nlC",
        "outputId": "1502af45-4391-485d-9472-d46675dc0acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"Pytorch version: \", torch.__version__)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version:  1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO-06Rlqk8W9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, time, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PWecmH8yui0"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCMW-mPvwrcw",
        "outputId": "c241552d-f024-410d-9926-fddf3ad24789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from google.colab import files, drive   \n",
        "\n",
        "# mount the google drive to my Colab session\n",
        "drive.mount('/content/gdrive')\n",
        "# use the google drive in my Colab session\n",
        "home_path = '/content/gdrive/My Drive/cs101'\n",
        "print(os.listdir(home_path))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['video_fluo_data', 'video_bf_data', 'processed', 'video_data_time_info_CS101.xlsx', 'ResNet50_v2-finetune-data_aug-order-random-dataaug.params', 'CS 101 Weekly Notes.gslides']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVfTd_Uhyui4"
      },
      "source": [
        "### Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jarQcT4Myge3",
        "outputId": "977c3710-5ae3-4b9b-9799-3c16a98a3cba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fixing the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Available high quality data\n",
        "embryo_inds = [1, 3, 12, 13, 16, 18, 19, 24, 39, 40, 42, 45, 46, 47, 49, 50, 52, 53]\n",
        "\n",
        "# Load info about videos\n",
        "video_time_info = pd.read_excel(f'{home_path}/video_data_time_info_CS101.xlsx', index_col=0, header=0, na_values=['NaN','NAN'], usecols=['usable_index', 'first_pol_time', 't_num'])  \n",
        "video_time_info.dropna(inplace=True, subset=['first_pol_time'])\n",
        "print(video_time_info.loc[embryo_inds])\n",
        "\n",
        "# Directory of the processed *.npy files\n",
        "processed_path = f'{home_path}/processed'\n",
        "polar_processed_path = f'{processed_path}/polarization'\n",
        "\n",
        "p = np.random.permutation(len(embryo_inds))\n",
        "p_embryo = [embryo_inds[i] for i in p]\n",
        "t_num = list(video_time_info.loc[embryo_inds, 't_num'])\n",
        "t_num_random = list(video_time_info.loc[p_embryo, 't_num'])\n",
        "\n",
        "instance_cum_random = np.cumsum(t_num_random)\n",
        "test_split_point = instance_cum_random[-1]*0.83\n",
        "temp = abs(instance_cum_random-test_split_point)\n",
        "test_idx = np.argmin(temp)\n",
        "\n",
        "val_split_point = instance_cum_random[-1]*0.7\n",
        "temp = abs(instance_cum_random-val_split_point)\n",
        "val_idx = np.argmin(temp)\n",
        "\n",
        "\n",
        "train_embryos = p_embryo[:val_idx]\n",
        "val_embryos = p_embryo[val_idx:test_idx]\n",
        "test_embryos = p_embryo[test_idx:]\n",
        "print(train_embryos)\n",
        "print(val_embryos)\n",
        "print(test_embryos)\n",
        "\n",
        "data_path = f'{processed_path}/fluo_data/middle'\n",
        "pol_path = f'{processed_path}/polarization'\n",
        "train_path = os.path.join(data_path, 'train')\n",
        "val_path = os.path.join(data_path, 'val')\n",
        "test_path = os.path.join(data_path, 'test')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              first_pol_time  t_num\n",
            "usable_index                       \n",
            "1                       17.0   21.0\n",
            "3                       12.0   21.0\n",
            "12                      11.0  143.0\n",
            "13                      12.0  143.0\n",
            "16                      30.0  143.0\n",
            "18                      27.0  143.0\n",
            "19                      32.0  143.0\n",
            "24                      20.0   21.0\n",
            "39                      12.0   21.0\n",
            "40                       8.0   21.0\n",
            "42                      16.0   21.0\n",
            "45                      19.0   21.0\n",
            "46                      19.0   21.0\n",
            "47                      17.0   21.0\n",
            "49                      21.0   21.0\n",
            "50                      21.0   21.0\n",
            "52                      13.0   21.0\n",
            "53                      20.0   21.0\n",
            "[1, 3, 39, 18, 13, 47, 52, 50, 45, 12, 40, 53]\n",
            "[16, 46, 24]\n",
            "[42, 49, 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAJrlvKDyui8"
      },
      "source": [
        "### Save NP Data as PNG for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DeDWzqZ1xKt"
      },
      "source": [
        "# Actually create the images\n",
        "\n",
        "def within_window(embryo_idx, t, window):\n",
        "    '''\n",
        "    embryo_idx: index of current embryo (value from 'usable_index' col)\n",
        "    t: timestep of current\n",
        "    window: number of t steps from first polarized index to ignore\n",
        "    '''\n",
        "    first_pol_idx = video_time_info.loc[embryo_idx, 'first_pol_time'] - 1\n",
        "    return window and abs(first_pol_idx - t) <= window\n",
        "\n",
        "def save_nps_as_png(embryos, save_path, window=None):\n",
        "    '''\n",
        "    embryos: subset of p_embryo... train, val, test\n",
        "    save_path: path to save png to... data_path + {'train', 'val', 'test'}\n",
        "    window: number of t steps from first polarized index to ignore\n",
        "    '''\n",
        "    for i in range(len(embryos)):\n",
        "        embryo_idx = embryos[i]\n",
        "        embryo_path = f'{data_path}/embryo_{embryo_idx}.npy'\n",
        "        embryo_pol_path = f'{pol_path}/embryo_{embryo_idx}.npy'\n",
        "        embryo = np.load(embryo_path)\n",
        "        embryo_pol = np.squeeze(np.load(embryo_pol_path)).astype(int)\n",
        "        embryo = embryo.astype(np.float64) / np.max(embryo) # normalize the data to 0 - 1\n",
        "        embryo = 255 * embryo # Now scale by 255\n",
        "        embryo = embryo.astype(np.uint8)\n",
        "        print(embryo_idx, np.shape(embryo)[2])\n",
        "        for t in range(np.shape(embryo)[2]):\n",
        "            if within_window(embryo_idx, t, window):\n",
        "                print(f'skipping embryo {embryo_idx} step {t}')\n",
        "                continue\n",
        "            pol = embryo_pol[t]\n",
        "            img = Image.fromarray(embryo[:,:,t], 'L')\n",
        "            img_path = f'{save_path}/{pol}/embryo_{embryo_idx}_{t}.png'\n",
        "            img.save(img_path)\n",
        "\n",
        "\n",
        "if not os.path.exists(train_path):\n",
        "    save_nps_as_png(train_embryos, train_path, window=None)\n",
        "if not os.path.exists(val_path):\n",
        "    save_nps_as_png(val_embryos, val_path, window=None)\n",
        "if not os.path.exists(test_path):\n",
        "    save_nps_as_png(test_embryos, test_path, window=None)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhHeN0l4CP5y"
      },
      "source": [
        "### Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQCpNXYCP53",
        "outputId": "8b4a5836-c8df-4b12-ae7a-62163c43f44f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classes = 2\n",
        "\n",
        "epochs = 15\n",
        "lr = 0.001\n",
        "per_device_batch_size = 64\n",
        "momentum = 0.9\n",
        "wd = 0.0001\n",
        "\n",
        "lr_factor = 0.75\n",
        "lr_steps = [10, 20, 30, np.inf]\n",
        "\n",
        "num_workers = 2\n",
        "num_gpus = 1\n",
        "gpus = [i for i in range(num_gpus)]\n",
        "\n",
        "dev = torch.device(gpus[0]) if num_gpus > 0 else [torch.device('cpu')]\n",
        "\n",
        "batch_size = per_device_batch_size * max(num_gpus, 1)\n",
        "print(dev)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THLMmIhfCP56"
      },
      "source": [
        "Things to keep in mind:\n",
        "\n",
        "1. ``epochs = 5`` is just for this tutorial with the tiny dataset. please change it to a larger number in your experiments, for instance 40.\n",
        "2. ``per_device_batch_size`` is also set to a small number. In your experiments you can try larger number like 64.\n",
        "3. remember to tune ``num_gpus`` and ``num_workers`` according to your machine.\n",
        "4. A pre-trained model is already in a pretty good status. So we can start with a small ``lr``.\n",
        "\n",
        "### Data Augmentation\n",
        "\n",
        "In transfer learning, data augmentation can also help.\n",
        "We use the following augmentation in training:\n",
        "\n",
        "2. Randomly crop the image and resize it to 224x224\n",
        "3. Randomly flip the image horizontally\n",
        "4. Randomly jitter color and add noise\n",
        "5. Transpose the data from height*width*num_channels to num_channels*height*width, and map values from [0, 255] to [0, 1]\n",
        "6. Normalize with the mean and standard deviation from the ImageNet dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgiHxUECP56"
      },
      "source": [
        "## Legacy mxnet transforms\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.Resize(600, keep_ratio=True),\n",
        "#     transforms.CenterCrop(512),\n",
        "\n",
        "#     transforms.RandomFlipLeftRight(), # Randomly flip the image horizontally\n",
        "#     transforms.RandomFlipTopBottom(),\n",
        "#     transforms.RandomLighting(0.1), # Add AlexNet-style PCA-based noise to an image\n",
        "#     transforms.RandomContrast(0.1),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.Resize(600, keep_ratio=True),\n",
        "#     transforms.CenterCrop(512),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# an example Torch transform:\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mRtApbiyujH"
      },
      "source": [
        "### Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwujcM-Qw8-B"
      },
      "source": [
        "First we define a subset of the ataset so it is faster to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Ob1yQmxCIM"
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "n_training_samples = 20 # Max: 50 000 - n_val_samples\n",
        "n_val_samples = 5\n",
        "n_test_samples = 5\n",
        "\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
        "# (In the last case, indexes do not need to account for training ones because the train=False parameter in datasets.CIFAR will select from the test set)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OKZ2hq1CP59"
      },
      "source": [
        "With the data augmentation functions, we can define our data loaders:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226lVHP2CP5-",
        "outputId": "aa60924d-f5d6-44d0-f626-0819d6be9dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# filelist = [f for f in os.listdir(train_path) if os.path.isfile(os.path.join(train_path, f))]\n",
        "# print(filelist)\n",
        "\n",
        "# TODO: Shuffle file list\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_path, transform=None, target_transform=None, loader=None, is_valid_file=None)\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_path, transform=None, target_transform=None, loader=None, is_valid_file=None)\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_path, transform=None, target_transform=None, loader=None, is_valid_file=None)\n",
        "\n",
        "print(\"Train data: \", train_data)\n",
        "print(\"Validation data: \", val_data)\n",
        "print(\"Test data: \", test_data)\n",
        "\n",
        "# Define data loaders\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, sampler=None, pin_memory=False, drop_last=False)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "classes = ('polaized', 'unpolarized')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data:  Dataset ImageFolder\n",
            "    Number of datapoints: 618\n",
            "    Root location: /content/gdrive/My Drive/cs101/processed/fluo_data/middle/train\n",
            "Validation data:  Dataset ImageFolder\n",
            "    Number of datapoints: 185\n",
            "    Root location: /content/gdrive/My Drive/cs101/processed/fluo_data/middle/val\n",
            "Test data:  Dataset ImageFolder\n",
            "    Number of datapoints: 185\n",
            "    Root location: /content/gdrive/My Drive/cs101/processed/fluo_data/middle/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ihNL_357yq"
      },
      "source": [
        "Visualize some training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCCEp0yzySSD",
        "outputId": "cbb6b61b-0a7f-4563-fd64-2fdabca26dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('{:>10}'.format(classes[labels[j]]) for j in range(test_batch_size)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-96c306182700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get some random training images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# show images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 137, in __getitem__\n    sample = self.loader(path)\nTypeError: 'NoneType' object is not callable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IkfDxPxCP6B"
      },
      "source": [
        "\n",
        "Note that only ``train_data`` uses ``transform_train``, while\n",
        "``val_data`` and ``test_data`` use ``transform_test`` to produce deterministic\n",
        "results for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxuE5F-eyujJ"
      },
      "source": [
        "### Define Model\n",
        "\n",
        "Simple CNN for now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLB3UfHJ3VUU"
      },
      "source": [
        "class SimpleConvolutionalNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleConvolutionalNetwork, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # cf comments in forward() to have step by step comments\n",
        "        # on the shape (how we pass from a 3x32x32 input image to a 18x16x16 volume)\n",
        "        self.fc1 = nn.Linear(18 * 16 * 16, 64) \n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass,\n",
        "        x shape is (batch_size, 3, 32, 32)\n",
        "        (color channel first)\n",
        "        in the comments, we omit the batch_size in the shape\n",
        "        \"\"\"\n",
        "        # shape : 3x32x32 -> 18x32x32\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # 18x32x32 -> 18x16x16\n",
        "        x = self.pool(x)\n",
        "        # 18x16x16 -> 4608\n",
        "        x = x.view(-1, 18 * 16 * 16)\n",
        "        # 4608 -> 64\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # 64 -> 10\n",
        "        # The softmax non-linearity is applied later (cf createLossAndOptimizer() fn)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzerQKjV3dKD"
      },
      "source": [
        "class LinearClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Linear Classifier\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(32 * 32 * 3, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten input 3x32x32 -> 3072\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaQn1kxIyujM"
      },
      "source": [
        "### Initialize Optimizer and Some Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw5oJ6PX3r8w"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    # it combines softmax with negative log likelihood loss\n",
        "    criterion = nn.CrossEntropyLoss()  \n",
        "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    return criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gVssLY630jD"
      },
      "source": [
        "# duplicate from before, but maybe it's useful to be able to change the batch size dynamically during training...? mini-batches?\n",
        "\n",
        "def get_train_loader(batch_size):\n",
        "    return torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler,\n",
        "                                              num_workers=num_workers)\n",
        "\n",
        "# Use larger batch size for validation to speed up computation\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, sampler=val_sampler,\n",
        "                                          num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJG2Rg_NCP6G"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Following is the main training loop. It is the same as the loop in\n",
        "`CIFAR10 <dive_deep_cifar10.html>`__\n",
        "and ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7mf0H-N4CmG"
      },
      "source": [
        "def train(net, batch_size, n_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a neural network and print statistics of the training\n",
        "    \n",
        "    :param net: (PyTorch Neural Network)\n",
        "    :param batch_size: (int)\n",
        "    :param n_epochs: (int)  Number of iterations on the training set\n",
        "    :param learning_rate: (float) learning rate used by the optimizer\n",
        "    \"\"\"\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"n_epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_minibatches = len(train_loader)\n",
        "\n",
        "    criterion, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    # Init variables used for plotting the loss\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    best_error = np.inf\n",
        "    best_model_path = \"best_model.pth\"\n",
        "\n",
        "\n",
        "    # mutli-gpu\n",
        "    if gpus is not None and len(gpus) > 1:\n",
        "        print('multi-gpu ', gpus)\n",
        "        net = torch.nn.DataParallel(net, device_ids=gpus)  # model becomes `torch.nn.DataParallel` w/ model.module being the orignal `torch.nn.Module`\n",
        "        net = net.to(dev)\n",
        "\n",
        "    # Move model to gpu if possible\n",
        "    else:\n",
        "        net = net.to(dev)\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        print_every = n_minibatches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            # Move tensors to correct device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # print every 10th of epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:    \n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
        "                      time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "\n",
        "        train_history.append(total_train_loss / len(train_loader))\n",
        "\n",
        "        total_val_loss = 0\n",
        "        # Do a pass on the validation set\n",
        "        # We don't need to compute gradient,\n",
        "        # we save memory and computation using torch.no_grad()\n",
        "        with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              # Move tensors to correct device\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              # Forward pass\n",
        "              predictions = net(inputs)\n",
        "              val_loss = criterion(predictions, labels)\n",
        "              total_val_loss += val_loss.item()\n",
        "            \n",
        "        val_history.append(total_val_loss / len(val_loader))\n",
        "        # Save model that performs best on validation set\n",
        "        if total_val_loss < best_error:\n",
        "            best_error = total_val_loss\n",
        "            torch.save(net.state_dict(), best_model_path)\n",
        "\n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "\n",
        "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    \n",
        "    # Load best model\n",
        "    net.load_state_dict(torch.load(best_model_path))\n",
        "    \n",
        "    return train_history, val_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8XS0ijo4ROm"
      },
      "source": [
        "net = SimpleConvolutionalNetwork()\n",
        "\n",
        "train_history, val_history = train(net, batch_size=32, n_epochs=10, learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4tY-o1fyujS"
      },
      "source": [
        "### Plot results (these need to be refactored)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCQdZHEayujT"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(0,epochs,1),train_acc_lst,'r')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('CNN Classify Fluo Polarization')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebW8cv5kyujU"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(0,epochs,1),train_loss_lst,'r')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('CNN Classify Fluo Polarization')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xWHwM3AyujV"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(0,epochs,1),val_acc_lst,'r')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('CNN Classify Fluo Polarization')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oxvgfhcHGX1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}