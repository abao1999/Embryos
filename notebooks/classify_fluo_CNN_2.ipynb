{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "classify_fluo_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs8XA3PzpPQL"
      },
      "source": [
        "# CNN for Fluo Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFVTssmYpPQP"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H29TGLErCP5r"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVkEhqIhCeLH",
        "outputId": "23b93a46-6ea9-46c3-e612-a97effbb5991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "!pip install mxnet-cu101\n",
        "!pip install gluoncv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n",
            "\u001b[K     |███████████████████████████████▌| 834.1MB 1.5MB/s eta 0:00:09tcmalloc: large alloc 1147494400 bytes == 0x662ea000 @  0x7f305212b615 0x591f47 0x4cc229 0x4cc38b 0x50a51c 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d\n",
            "\u001b[K     |████████████████████████████████| 846.0MB 21kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.18.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n",
            "Collecting gluoncv\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/d7/74b530c461ac3eb90f6045a645a59450de1f3d616a4926e371daa021dbd8/gluoncv-0.8.0-py2.py3-none-any.whl (810kB)\n",
            "\u001b[K     |████████████████████████████████| 819kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from gluoncv) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gluoncv) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gluoncv) (1.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gluoncv) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gluoncv) (2.23.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gluoncv) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->gluoncv) (1.15.0)\n",
            "Installing collected packages: portalocker, gluoncv\n",
            "Successfully installed gluoncv-0.8.0 portalocker-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG53T9hI2nlC"
      },
      "source": [
        "import mxnet as mx\n",
        "import numpy as np\n",
        "import os, time, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from mxnet import gluon, image, init, nd\n",
        "from mxnet import autograd as ag\n",
        "from mxnet.gluon import nn\n",
        "from mxnet.gluon.data.vision import transforms\n",
        "from gluoncv.utils import makedirs\n",
        "from gluoncv.model_zoo import get_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEmRYeu7pPQt"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCMW-mPvwrcw",
        "outputId": "da3061f5-537f-43c0-c25d-9e3146fb2c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import os\n",
        "from google.colab import files, drive   \n",
        "import pandas as pd\n",
        "\n",
        "# mount the google drive to my Colab session\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "home_path = '/content/gdrive/Shared drives/Embryo_data'\n",
        "\n",
        "# use the google drive in my Colab session\n",
        "print(os.listdir(home_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['mxnet_cnn2d_embryo_58_fine_tune_data_aug_ResNet50_v2_order_random.ipynb', 'Embryo1', 'Embryo3', 'Embryo12', 'Embryo13', 'Embryo16', 'Embryo20', 'Embryo19', 'Embryo18', 'Embryo24', 'Embryo40', 'Embryo39', 'Embryo42', 'Embryo52', 'Embryo50', 'Embryo49', 'Embryo45', 'Embryo46', 'Embryo47', 'Embryo23', 'Embryo33', 'Embryo25', 'Embryo95', 'Embryo97', 'Embryo96', 'Embryo98', 'Embryo101', 'Embryo99', 'Embryo100', 'Embryo102', 'Embryo76', 'Embryo78', 'Embryo81', 'Embryo79', 'Embryo80', 'Embryo82', 'Embryo84', 'Embryo83', 'Embryo85', 'Embryo87', 'Embryo88', 'Embryo92', 'Embryo94', 'Embryo93', 'raw', 'embryo_info_CS101.xlsx']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLhQqeUxpPQ3"
      },
      "source": [
        "### Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jarQcT4Myge3",
        "outputId": "097c89bb-ba13-436e-ede3-3c2d9b282dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Fixing the random seed\n",
        "mx.random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load info about videos\n",
        "cols_to_skip = ['remarks']\n",
        "video_time_info = pd.read_excel(f'{home_path}/embryo_info_CS101.xlsx', usecols=lambda x: x not in cols_to_skip)\n",
        "\n",
        "# Fill list of embryo indices and corresponding total t val\n",
        "embryo_inds = []\n",
        "t_nums = []\n",
        "for pd_idx in range(len(video_time_info)):\n",
        "    # print(f'{pd_idx}/{len(video_time_info)}')\n",
        "    if all(video_time_info[['if_full_injected', 'fluo_quality_of_z_max_sum', 'fluo_quality_of_raw_png', 'if_healthy']].to_numpy()[pd_idx]):\n",
        "        # embryo_idx, t_num, c_fluo, c_bf = embryo_data[[\"embryo_index\", \"t_num\", \"fluo_channel\", \"DIC_channel\"]].values[pd_idx]\n",
        "        embryo_idx, t_num = video_time_info[['embryo_index', 't_num']].to_numpy()[pd_idx]\n",
        "        embryo_inds.append(embryo_idx)\n",
        "        t_nums.append(t_num)\n",
        "\n",
        "# t_num = video_time_info.loc[:,'t_num'].to_numpy()\n",
        "print(\"Embryo indices: \", embryo_inds)\n",
        "print(\"Corresponding total t:\", list(t_nums))\n",
        "assert len(embryo_inds) == len(t_nums)\n",
        "\n",
        "embryo_inds = np.array(embryo_inds)\n",
        "t_nums = np.array(t_nums)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embryo indices:  [1, 3, 12, 13, 16, 18, 19, 20, 24, 39, 40, 42, 45, 46, 47, 49, 50, 52, 76, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]\n",
            "Corresponding total t: [21, 21, 143, 143, 143, 143, 143, 143, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arr3W8MvMUtx",
        "outputId": "f02a1f9f-12b4-41e6-9b83-e0a9e6fa1483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "## Train-Val-Test Split\n",
        "\n",
        "# Randomly shuffle\n",
        "p = np.random.permutation(len(embryo_inds))\n",
        "embryo_inds_rand = np.zeros_like(embryo_inds)\n",
        "t_nums_rand = np.zeros_like(t_nums)\n",
        "\n",
        "for i in range(len(embryo_inds)):\n",
        "  embryo_inds_rand[i] = embryo_inds[p[i]]\n",
        "  t_nums_rand[i] = t_nums[p[i]]\n",
        "\n",
        "t_nums_rand_cumsum = np.cumsum(t_nums_rand)\n",
        "# print(t_nums_rand_cumsum)\n",
        "\n",
        "test_split_point = t_nums_rand_cumsum[-1]*0.83\n",
        "temp = abs(t_nums_rand_cumsum-test_split_point)\n",
        "test_idx = np.argmin(temp)\n",
        "print(\"test idx: \", test_idx)\n",
        "\n",
        "val_split_point = t_nums_rand_cumsum[-1]*0.7\n",
        "temp = abs(t_nums_rand_cumsum-val_split_point)\n",
        "val_idx = np.argmin(temp)\n",
        "print(\"val idx: \", val_idx)\n",
        "\n",
        "train_embryos = embryo_inds_rand[:val_idx]\n",
        "val_embryos = embryo_inds_rand[val_idx:test_idx]\n",
        "test_embryos = embryo_inds_rand[test_idx:]\n",
        "print(train_embryos)\n",
        "print(val_embryos)\n",
        "print(test_embryos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test idx:  31\n",
            "val idx:  26\n",
            "[ 81   1  85  16  95  76  40  96  49  45  94  39  82  18  80  93  92  98\n",
            " 100  99  50  97  12  88  20  42]\n",
            "[19 47 87 78  3]\n",
            "[ 79  84 102  46  83  13  52 101  24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B3r55kIMYT8"
      },
      "source": [
        "# Parent directories of the processed *.npy files\n",
        "processed_path = f'{home_path}/processed'\n",
        "polar_processed_path = f'{processed_path}/polarization'\n",
        "\n",
        "# Directories to load data from\n",
        "data_path = f'{processed_path}/fluo_data/middle'\n",
        "pol_path = f'{processed_path}/polarization'\n",
        "\n",
        "# Directories to write to\n",
        "train_path = os.path.join(data_path, 'train')\n",
        "val_path = os.path.join(data_path, 'val')\n",
        "test_path = os.path.join(data_path, 'test')\n",
        "\n",
        "\n",
        "# TODO: get middle dynamically, or re-make subdirectory structure in the new drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sykA02VwpPRA"
      },
      "source": [
        "### Save NP Data as PNG for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DeDWzqZ1xKt",
        "outputId": "26ac1fa4-585c-413e-b195-713faca7b575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# Actually create the images\n",
        "\n",
        "def save_nps_as_png(embryos, save_path):\n",
        "    '''\n",
        "    embryos: subset of embryo_inds_rand... train, val, test\n",
        "    save_path: path to save png to... data_path + {'train', 'val', 'test'}\n",
        "    '''\n",
        "    for i in range(len(embryos)):\n",
        "        embryo_idx = embryos[i]\n",
        "        embryo_path = f'{data_path}/embryo_{embryo_idx}.npy'\n",
        "        embryo_pol_path = f'{pol_path}/embryo_{embryo_idx}.npy'\n",
        "        embryo = np.load(embryo_path)\n",
        "        embryo_pol = np.squeeze(np.load(embryo_pol_path)).astype(int)\n",
        "        embryo = embryo.astype(np.float64) / np.max(embryo) # normalize the data to 0 - 1\n",
        "        embryo = 255 * embryo # Now scale by 255\n",
        "        embryo = embryo.astype(np.uint8)\n",
        "        print(embryo_idx)\n",
        "        for t in range(np.shape(embryo)[2]):\n",
        "            pol = embryo_pol[t]\n",
        "            img = Image.fromarray(embryo[:,:,t], 'L')\n",
        "            img_path = f'{save_path}/{pol}/embryo_{embryo_idx}_{t}.png'\n",
        "            img.save(img_path)\n",
        "\n",
        "save_nps_as_png(train_embryos, train_path)\n",
        "save_nps_as_png(val_embryos, val_path)\n",
        "save_nps_as_png(test_embryos, test_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-d22f981d801d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msave_nps_as_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embryos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0msave_nps_as_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_embryos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0msave_nps_as_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embryos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-d22f981d801d>\u001b[0m in \u001b[0;36msave_nps_as_png\u001b[0;34m(embryos, save_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membryo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{data_path}/embryo_{embryo_idx}.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0membryo_pol_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{pol_path}/embryo_{embryo_idx}.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0membryo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membryo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0membryo_pol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membryo_pol_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0membryo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membryo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membryo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# normalize the data to 0 - 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/Shared drives/Embryo_data/processed/fluo_data/middle/embryo_78.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhHeN0l4CP5y"
      },
      "source": [
        "### Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQCpNXYCP53",
        "outputId": "f7d019c8-8afd-42a3-cf44-c9ef283faea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classes = 2\n",
        "\n",
        "epochs = 15\n",
        "lr = 0.001\n",
        "per_device_batch_size = 16\n",
        "momentum = 0.9\n",
        "wd = 0.0001\n",
        "\n",
        "lr_factor = 0.75\n",
        "lr_steps = [10, 20, 30, np.inf]\n",
        "\n",
        "num_gpus = 1\n",
        "num_workers = 8\n",
        "ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
        "batch_size = per_device_batch_size * max(num_gpus, 1)\n",
        "print(ctx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[gpu(0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THLMmIhfCP56"
      },
      "source": [
        "Things to keep in mind:\n",
        "\n",
        "1. ``epochs = 5`` is just for this tutorial with the tiny dataset. please change it to a larger number in your experiments, for instance 40.\n",
        "2. ``per_device_batch_size`` is also set to a small number. In your experiments you can try larger number like 64.\n",
        "3. remember to tune ``num_gpus`` and ``num_workers`` according to your machine.\n",
        "4. A pre-trained model is already in a pretty good status. So we can start with a small ``lr``.\n",
        "\n",
        "### Data Augmentation\n",
        "\n",
        "In transfer learning, data augmentation can also help.\n",
        "We use the following augmentation in training:\n",
        "\n",
        "2. Randomly crop the image and resize it to 224x224\n",
        "3. Randomly flip the image horizontally\n",
        "4. Randomly jitter color and add noise\n",
        "5. Transpose the data from height*width*num_channels to num_channels*height*width, and map values from [0, 255] to [0, 1]\n",
        "6. Normalize with the mean and standard deviation from the ImageNet dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgiHxUECP56"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(600, keep_ratio=True),\n",
        "    transforms.CenterCrop(512),\n",
        "\n",
        "    transforms.RandomFlipLeftRight(), # Randomly flip the image horizontally\n",
        "    transforms.RandomFlipTopBottom(),\n",
        "    transforms.RandomLighting(0.1), # Add AlexNet-style PCA-based noise to an image\n",
        "    transforms.RandomContrast(0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(600, keep_ratio=True),\n",
        "    transforms.CenterCrop(512),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1gV7pNypPRV"
      },
      "source": [
        "### Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OKZ2hq1CP59"
      },
      "source": [
        "With the data augmentation functions, we can define our data loaders:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226lVHP2CP5-",
        "outputId": "2f99ecc8-093e-4daf-ed99-db2fc1abfb9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = gluon.data.DataLoader(\n",
        "    gluon.data.vision.ImageFolderDataset(train_path).transform_first(transform_train),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "val_data = gluon.data.DataLoader(\n",
        "    gluon.data.vision.ImageFolderDataset(val_path).transform_first(transform_train),\n",
        "    batch_size=batch_size, shuffle=True, num_workers = num_workers)\n",
        "\n",
        "test_data = gluon.data.DataLoader(\n",
        "    gluon.data.vision.ImageFolderDataset(test_path).transform_first(transform_test),\n",
        "    batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
        "\n",
        "print(len(train_data) + len(val_data) + len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IkfDxPxCP6B"
      },
      "source": [
        "Note that only ``train_data`` uses ``transform_train``, while\n",
        "``val_data`` and ``test_data`` use ``transform_test`` to produce deterministic\n",
        "results for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLmQeaAypPRf"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idrl6BrBpPRh"
      },
      "source": [
        "### Define Model\n",
        "\n",
        "Simple CNN for now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRZ4b3KxpPRj"
      },
      "source": [
        "import mxnet.ndarray as F\n",
        "\n",
        "class Net(gluon.Block):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Net, self).__init__(**kwargs)\n",
        "        with self.name_scope():\n",
        "            # layers created in name_scope will inherit name space\n",
        "            # from parent layer.\n",
        "            self.conv1 = nn.Conv2D(20, kernel_size=(5,5))\n",
        "            self.pool1 = nn.MaxPool2D(pool_size=(2,2), strides = (2,2))\n",
        "            self.conv2 = nn.Conv2D(50, kernel_size=(5,5))\n",
        "            self.pool2 = nn.MaxPool2D(pool_size=(2,2), strides = (2,2))\n",
        "            self.fc1 = nn.Dense(500)\n",
        "            self.fc2 = nn.Dense(10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.tanh(self.conv1(x)))\n",
        "        x = self.pool2(F.tanh(self.conv2(x)))\n",
        "        # 0 means copy over size from corresponding dimension.\n",
        "        # -1 means infer size from the rest of dimensions.\n",
        "        x = x.reshape((0, -1))\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = F.tanh(self.fc2(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPkUH0gpPRu"
      },
      "source": [
        "### Initialize Model, Parameters, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X6_lGL4pPRw"
      },
      "source": [
        "net = Net()\n",
        "\n",
        "optimizer = 'sgd' # 'nag': Nesterov accelerated gradient descent\n",
        "optimizer_params = {'learning_rate': lr, 'wd': wd, 'momentum': momentum} # Set parameters\n",
        "\n",
        "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-LoH0gppPR2"
      },
      "source": [
        "### Define Evaluation Function for Test/Val Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzIMoYwlCP6E"
      },
      "source": [
        "def test(net, val_data, ctx):\n",
        "    metric = mx.metric.Accuracy()\n",
        "    for i, batch in enumerate(val_data):\n",
        "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        outputs = [net(X) for X in data]\n",
        "        metric.update(label, outputs)\n",
        "\n",
        "    return metric.get()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJG2Rg_NCP6G"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Following is the main training loop. It is the same as the loop in\n",
        "`CIFAR10 <dive_deep_cifar10.html>`__\n",
        "and ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwRaiiomCP6H",
        "outputId": "90aec84b-f3d3-4fcc-bc9f-9434216e30f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lr_counter = 0\n",
        "num_batch = len(train_data)\n",
        "train_acc_lst = []\n",
        "train_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    if epoch == lr_steps[lr_counter]:\n",
        "        trainer.set_learning_rate(trainer.learning_rate*lr_factor)\n",
        "        lr_counter += 1\n",
        "\n",
        "    tic = time.time()\n",
        "    train_loss = 0\n",
        "    metric.reset()\n",
        "\n",
        "    for i, batch in enumerate(train_data):\n",
        "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        with ag.record():\n",
        "            outputs = [net(X) for X in data]\n",
        "            loss = [L(yhat, y) for yhat, y in zip(outputs, label)]\n",
        "        for l in loss:\n",
        "            l.backward()\n",
        "\n",
        "        trainer.step(batch_size)\n",
        "        train_loss += sum([l.mean().asscalar() for l in loss]) / len(loss)\n",
        "\n",
        "        metric.update(label, outputs)\n",
        "\n",
        "    _, train_acc = metric.get()\n",
        "    train_loss /= num_batch\n",
        "\n",
        "    _, val_acc = test(net, val_data, ctx)\n",
        "\n",
        "    train_acc_lst.append(train_acc)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    val_acc_lst.append(val_acc)\n",
        "    print('[Epoch %d] Train-acc: %.3f, loss: %.3f | Val-acc: %.3f | time: %.1f' %\n",
        "             (epoch, train_acc, train_loss, val_acc, time.time() - tic))\n",
        "\n",
        "_, test_acc = test(net, test_data, ctx)\n",
        "print('[Finished] Test-acc: %.3f' % (test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0] Train-acc: 0.992, loss: 0.028 | Val-acc: 0.941 | time: 37.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzadbNPUpPSC"
      },
      "source": [
        "### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNrORWiFpPSD"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(0,epochs,1),test_acc_lst,'r')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('CNN Classify Fluo Polarization')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRSMeNeApPSI"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(0,epochs,1),test_loss_lst,'r')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('CNN Classify Fluo Polarization')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFFYJnzbpPSN"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(0,epochs,1),val_acc_lst,'r')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('CNN Classify Fluo Polarization')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}