{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix_PyTorch-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abao1999/Embryos/blob/main/notebooks/pix2pix_PyTorch_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jomokfQvqafF"
      },
      "source": [
        "Adapted from https://github.com/eriklindernoren/PyTorch-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPUXnAXhxrcw"
      },
      "source": [
        "###Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoTPBYPhfJ70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d5b7ea-7660-4dcb-f820-439e644a3b09"
      },
      "source": [
        "import os\n",
        "from google.colab import files, drive   \n",
        "\n",
        "# mount the google drive to my Colab session\n",
        "drive.mount('/content/gdrive')\n",
        "# use the google drive in my Colab session\n",
        "print(os.listdir('/content/gdrive/My Drive/'))\n",
        "\n",
        "shared_path = '/content/gdrive/Shared drives/Embryo_data'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "['cs101', 'Getting started.pdf', 'Colab Notebooks', 'embryo_13_fluo_sequence.mp4', 'embryo_13_fluo_slice.mp4', 'embryo_13_bf_slice.mp4', 'embryo_13_bf_sequence.mp4', 'hi', 'embryo data.gsheet', 'classify_fluo_pretrained.ipynb', 'classify_fluo_CNN_2.ipynb', 'Copy of Pyramid Principle Exercise.gslides', 'dloss_100.npy', 'gloss_100.npy', 'generator_100.pth', 'discriminator_100.pth', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (8)', 'ResNet50_v2-batch16-lr0.001-mom0 (2).9-wd0.0001-epoch0.params', 'ResNet50_v2-batch16-lr0.001-mom0 (2).9-wd0.0001-epoch1.params', 'ResNet50_v2-batch16-lr0.001-mom0 (1).9-wd0.0001-epoch2.params', 'ResNet50_v2-batch16-lr0.001-mom0 (1).9-wd0.0001-epoch3.params', 'ResNet50_v2-batch16-lr0.001-mom0 (1).9-wd0.0001-epoch4.params', 'ResNet50_v2-batch16-lr0.001-mom0 (1).9-wd0.0001-epoch5.params', 'ResNet50_v2-batch16-lr0.001-mom0 (1).9-wd0.0001-epoch6.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (7)', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (6)', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (5)', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (4)', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (3)', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (2)', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001 (1)', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001', 'ResNet50_v2-batch16-lr0.001-mom0 (1).9-wd0.0001-epoch0.params', 'ResNet50_v2-batch16-lr0.001-mom0 (1).9-wd0.0001-epoch1.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch0.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch1.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch2.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch3.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch4.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch5.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch6.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch7.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch8.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch9.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch10.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch11.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch12.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch13.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch14.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch15.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch16.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch17.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch18.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch19.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch20.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch21.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch22.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch23.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch24.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch25.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch26.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch27.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch28.params', 'ResNet50_v2-batch16-lr0.001-mom0.9-wd0.0001-epoch29.params']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOGeGOGsNBUj"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JOQNRBSMZx2"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.files = sorted(glob.glob(os.path.join(root, mode, 'fluo_data') + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fn_fluo = self.files[index % len(self.files)]\n",
        "        fn_bf = fn_fluo.replace('fluo_data', 'bf_data')\n",
        "\n",
        "        img_fluo = Image.open(fn_fluo)\n",
        "        img_bf = Image.open(fn_bf)\n",
        "        img_fluo = self.transform(img_fluo)\n",
        "        img_bf = self.transform(img_bf)\n",
        "\n",
        "        ind = fn_fluo.find('embryo')\n",
        "        [embryo_num, embryo_t, label] = [int(num) for num in fn_fluo[ind:-4].split('_')[1:]]\n",
        "\n",
        "        return {\"bf\": img_bf, \"fluo\": img_fluo, \"embryo_num\": embryo_num, \"embryo_t\": embryo_t, \"label\": label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c645lKqNL7v"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YoE3VjHaBp9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N4CJ8XjyN4o"
      },
      "source": [
        "### UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvU8ht58yRN_"
      },
      "source": [
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "        super(UNetDown, self).__init__()\n",
        "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_size))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.0):\n",
        "        super(UNetUp, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he3scenGyXl_"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C12PYQVbyaTL"
      },
      "source": [
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # U-Net generator with skip connections from encoder to decoder\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "\n",
        "        return self.final(u7)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMnGNWIzyfZ8"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-u_3IaRyh0Q"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate image and condition image by channels to produce input\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        return self.model(img_input)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZueGJrtyFsH"
      },
      "source": [
        "## pix2pix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iITIH4P1hFE"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import itertools\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3uXFqlszwVk",
        "outputId": "f399ccbd-52e8-4c88-99cd-6a5f4bbb7cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "class Args():\n",
        "  def __init__(self, epoch=0, n_epochs=100, dataset_name='data', batch_size=1,\n",
        "          lr=0.0002, b1=0.5, b2=0.999, decay_epoch=100, n_cpu=8,\n",
        "          img_height=512, img_width=512, channels=1,\n",
        "          sample_interval=500, checkpoint_interval=10\n",
        "          ):\n",
        "      self.epoch = epoch # epoch to start training from\n",
        "      self.n_epochs = n_epochs # number of epochs of training\n",
        "      self.dataset_name = dataset_name # name of the dataset\n",
        "      self.batch_size = batch_size # size of the batches\n",
        "      self.lr = lr # adam: learning rate\n",
        "      self.b1 = b1 # adam: decay of first order momentum of gradient\n",
        "      self.b2 = b2 # adam: decay of second order momentum of gradient\n",
        "      self.decay_epoch = decay_epoch # epoch from which to start lr decay\n",
        "      self.n_cpu = n_cpu # number of cpu threads to use during batch generation\n",
        "      self.img_height = img_height # size of image height\n",
        "      self.img_width = img_width # size of image width\n",
        "      self.channels = channels # number of image channels\n",
        "      self.sample_interval = sample_interval # interval between sampling of images from generators\n",
        "      self.checkpoint_interval = checkpoint_interval # interval between model checkpoints\n",
        "\n",
        "opt = Args(dataset_name=\"pix2pix-normal-loss\")\n",
        "\n",
        "os.makedirs(f\"{shared_path}/images/{opt.dataset_name}\", exist_ok=True)\n",
        "os.makedirs(f\"{shared_path}/models/{opt.dataset_name}\", exist_ok=True)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "print(cuda)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJTZ8JczzRR"
      },
      "source": [
        "### Define loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhJ7qrhug_jd"
      },
      "source": [
        "# Loss functions\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_pixelwise = torch.nn.L1Loss()\n",
        "criterion_polarity = torch.nn.L1Loss()\n",
        "\n",
        "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
        "lambda_pixel = 1000\n",
        "lambda_polarity = 1000\n",
        "\n",
        "# Calculate output of image discriminator (PatchGAN)\n",
        "patch = (1, opt.img_height // 2 ** 4, opt.img_width // 2 ** 4)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDNXhtSJ3N6h"
      },
      "source": [
        "#### Generator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f56LiJ3POPp1",
        "outputId": "cca23236-780f-4da0-f7ef-4e31ff9de443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "classes = 2\n",
        "model_name = 'ResNet50_v2'\n",
        "num_gpus = 1\n",
        "ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
        "print(ctx)\n",
        "\n",
        "# Fluo\n",
        "finetune_net_fluo = get_model(model_name, pretrained=True)\n",
        "with finetune_net_fluo.name_scope():\n",
        "    finetune_net_fluo.output = mx.gluon.nn.Dense(classes)\n",
        "\n",
        "finetune_net_fluo.output.initialize(init.Xavier(), ctx=ctx)\n",
        "finetune_net_fluo.collect_params().reset_ctx(ctx)\n",
        "finetune_net_fluo.hybridize()\n",
        "\n",
        "model_fp = os.path.join(shared_path, 'models', 'best', 'best-fluo.params')\n",
        "finetune_net_fluo.load_parameters(model_fp, ctx=ctx)\n",
        "\n",
        "# Bf\n",
        "finetune_net_bf = get_model(model_name, pretrained=True)\n",
        "with finetune_net_bf.name_scope():\n",
        "    finetune_net_bf.output = mx.gluon.nn.Dense(classes)\n",
        "\n",
        "finetune_net_bf.output.initialize(init.Xavier(), ctx=ctx)\n",
        "finetune_net_bf.collect_params().reset_ctx(ctx)\n",
        "finetune_net_bf.hybridize()\n",
        "\n",
        "model_fp_bf = os.path.join(shared_path, 'models', 'best', 'best-bf.params')\n",
        "finetune_net_bf.load_parameters(model_fp_bf, ctx=ctx)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-451767c188eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ResNet50_v2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_gpus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-451767c188eb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ResNet50_v2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_gpus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6uHxOJRQTNZ"
      },
      "source": [
        "dummy_input = torch.from_numpy(np.zeros((1,3,512,512))).float().cuda()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkbWLGRG2JUG"
      },
      "source": [
        "# valid: all 1s\n",
        "def calculate_gen_loss(pred_fake, fake_fluo, real_fluo, valid):\n",
        "    # How well did the generator trick discriminator\n",
        "    loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "\n",
        "    # Pixel-wise loss\n",
        "    loss_pixel = criterion_pixelwise(fake_fluo, real_fluo)\n",
        "\n",
        "    # print(loss_GAN, loss_pixel)\n",
        "\n",
        "    # # Compare predicted label on fake image to ground truth label\n",
        "    # fake_fluo_mx = mx.nd.array(fake_fluo.cpu().numpy(), ctx=ctx[0])\n",
        "    # pred_label_fake = finetune_net_fluo(fake_fluo_mx)\n",
        "    # real_bf_mx = mx.nd.array(real_bf.cpu().numpy(), ctx=ctx[0])\n",
        "    # pred_label_real_bf = finetune_net_bf(real_bf)\n",
        "\n",
        "    # loss_polarity = criterion_polarity(pred_label_fake, label) + criterion_polarity(pred_label_fake, pred_label_real_bf)\n",
        "\n",
        "    # Total loss\n",
        "    loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
        "    # loss_G = loss_GAN + lambda_pixel * loss_pixel + lambda_polarity * loss_polarity\n",
        "    return loss_G, loss_pixel, loss_GAN"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l48b4BD-3QV3"
      },
      "source": [
        "#### Discriminator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB_K7apk3R5s"
      },
      "source": [
        "# valid: all 1s\n",
        "# fake: all 0s\n",
        "def calculate_disc_loss(pred_real, valid, pred_fake, fake):\n",
        "    # Real loss\n",
        "    loss_real = criterion_GAN(pred_real, valid)\n",
        "\n",
        "    # Fake loss\n",
        "    loss_fake = criterion_GAN(pred_fake, fake)\n",
        "\n",
        "    # Total loss\n",
        "    loss_D = 0.5 * (loss_real + loss_fake)\n",
        "    return loss_D"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTEg-Ce8z9mC"
      },
      "source": [
        "### Initialize models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyT3ykxJ0CqC"
      },
      "source": [
        "# Initialize generator and discriminator\n",
        "generator = GeneratorUNet(in_channels=1, out_channels=1)\n",
        "discriminator = Discriminator(in_channels=1)\n",
        "\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_pixelwise.cuda()\n",
        "\n",
        "if opt.epoch != 0:\n",
        "    # Load pretrained models\n",
        "    generator.load_state_dict(torch.load(f\"{shared_path}/models/{opt.dataset_name}/generator_{opt.epoch}.pth\"))\n",
        "    discriminator.load_state_dict(torch.load(f\"{shared_path}/models/{opt.dataset_name}/discriminator_{opt.epoch}.pth\"))\n",
        "else:\n",
        "    # Initialize weights\n",
        "    generator.apply(weights_init_normal)\n",
        "    discriminator.apply(weights_init_normal)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa6DPjDx0Jx0"
      },
      "source": [
        "### Configure data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7kdaAXd0Msf"
      },
      "source": [
        "transforms_ = [\n",
        "    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n",
        "    transforms.ToTensor()\n",
        "]\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(f\"{shared_path}/processed/pix2pix\", transforms_=transforms_),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(f\"{shared_path}/processed/pix2pix\", transforms_=transforms_, mode=\"val\"),\n",
        "    batch_size=10,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Tm2g-C61WR"
      },
      "source": [
        "# Tensor type\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "def sample_images(batches_done):\n",
        "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    real_A = Variable(imgs[\"bf\"].type(Tensor))\n",
        "    real_B = Variable(imgs[\"fluo\"].type(Tensor))\n",
        "    fake_B = generator(real_A)\n",
        "    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
        "    save_image(img_sample, f\"{shared_path}/images/{opt.dataset_name}/{batches_done}.png\", nrow=10, normalize=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeF1WhNp0xe6"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY-6N3W70zbf",
        "outputId": "90c5c1b0-1f76-4b1e-dbda-381ced270af5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prev_time = time.time()\n",
        "\n",
        "G_loss = []\n",
        "D_loss = []\n",
        "\n",
        "for epoch in range(opt.epoch, opt.n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        # Model inputs\n",
        "        real_A = Variable(batch[\"bf\"].type(Tensor))\n",
        "        real_B = Variable(batch[\"fluo\"].type(Tensor))\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = generator(real_A)\n",
        "        pred_fake = discriminator(fake_B, real_A)\n",
        "        loss_G, loss_pixel, loss_GAN = calculate_gen_loss(pred_fake, fake_B, real_B, valid)\n",
        "\n",
        "        loss_G.backward()\n",
        "\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        pred_real = discriminator(real_B, real_A)\n",
        "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
        "        loss_D = calculate_disc_loss(pred_real, valid, pred_fake, fake)\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "\n",
        "        # Determine approximate time left\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
        "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # Print log\n",
        "        sys.stdout.write(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
        "            % (\n",
        "                epoch,\n",
        "                opt.n_epochs,\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D.item(),\n",
        "                loss_G.item(),\n",
        "                loss_pixel.item(),\n",
        "                loss_GAN.item(),\n",
        "                time_left,\n",
        "            )\n",
        "        )\n",
        "        G_loss.append(loss_G.item())\n",
        "        D_loss.append(loss_D.item())\n",
        "\n",
        "        # If at sample interval save image\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            sample_images(batches_done)\n",
        "\n",
        "    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
        "        # Save model checkpoints\n",
        "        torch.save(generator.state_dict(), f\"{shared_path}/models/{opt.dataset_name}/generator_{epoch}.pth\")\n",
        "        torch.save(discriminator.state_dict(), f\"{shared_path}/models/{opt.dataset_name}/discriminator_{epoch}.pth\")\n",
        "\n",
        "torch.save(generator.state_dict(), f\"{shared_path}/models/{opt.dataset_name}/generator_{epoch}.pth\")\n",
        "torch.save(discriminator.state_dict(), f\"{shared_path}/models/{opt.dataset_name}/discriminator_{epoch}.pth\")\n",
        "\n",
        "D_loss = np.array(D_loss)\n",
        "G_loss = np.array(G_loss)\n",
        "np.save(f\"{shared_path}/models/{opt.dataset_name}/dloss_{opt.n_epochs}.npy\", D_loss)\n",
        "np.save(f\"{shared_path}/models/{opt.dataset_name}/gloss_{opt.n_epochs}.npy\", G_loss)  "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 61/100] [Batch 976/1900] [D loss: 0.000168] [G loss: 22.878229, pixel: 0.021889, adv: 0.988760] ETA: 3:14:24.664222Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-EUvC5p1LDI"
      },
      "source": [
        "### Plot loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLhPU5WtM3kv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b52715d0-9f2c-4ec8-f256-14afde748c34"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(D_loss[0:-1:918])\n",
        "plt.plot(G_loss[0:-1:918])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f28768c5048>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}